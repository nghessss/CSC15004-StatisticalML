{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOcWj_hWOX4T",
    "outputId": "3e3fdf81-d66e-4828-98cd-17859f6fe7f4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet fastapi uvicorn pyngrok fvcore transformers torchaudio librosa soundfile ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "import cv2\n",
    "import numpy as np\n",
    "import timm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import roi_align\n",
    "\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import JSONResponse, FileResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pyngrok import ngrok\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "import uuid\n",
    "import threading, uvicorn, logging, sys\n",
    "folder_path = '/kaggle/input/utils/other/default/21'\n",
    "print(os.listdir(folder_path))\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import object detection\n",
    "from object_detection import yolo_extract\n",
    "print(\"load od done\")\n",
    "# library for video motion\n",
    "from config import TrainConfig as C\n",
    "from models.abd_transformer import ABDTransformer\n",
    "import requests\n",
    "from utils import dict_to_cls\n",
    "from image_captioning import set_model_dict, generate_caption\n",
    "\n",
    "# import for image captioning\n",
    "from image_captioning import set_model_dict, generate_caption\n",
    "from keyframe_extraction import extract, clean_path, clean_file\n",
    "state_dict_path = \"/kaggle/input/vit-t5-image-captioning/pytorch/default/1/best_model_weights.pth\"\n",
    "set_model_dict(state_dict_path)\n",
    "print('load ic done')\n",
    "# import for split_video\n",
    "from split_video import split_media_ffmpeg\n",
    "print('load vm done')\n",
    "# import for audio transcript\n",
    "from audio import get_transcript\n",
    "\n",
    "print('load at done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries for Video Captioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For video captioning\n",
    "from loader.MSVD import MSVD\n",
    "from config import TrainConfig as C\n",
    "from models.abd_transformer import ABDTransformer\n",
    "import torch\n",
    "from utils import dict_to_cls\n",
    "# Inception-ResNet-V2 for image feature extraction\n",
    "import os\n",
    "import cv2 # Thêm cv2 để đọc video\n",
    "import numpy as np # Thêm numpy để xử lý mảng\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "# I3D for motion feature extraction\n",
    "from models.i3d.extract_i3d import ExtractI3D\n",
    "from video_feature_utils.utils import build_cfg_path\n",
    "from omegaconf import OmegaConf\n",
    "# Mask R-CNN for object feature extraction\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Thiết lập chung\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEATURE_MODE = \"three\"\n",
    "DEVICE, FEATURE_MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model for feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Tải mô hình Inception-ResNet-V2 ---\n",
    "print(\"Loading Inception-ResNet-V2 model...\")\n",
    "# Load Inception-ResNet-V2 (pretrained) từ timm cho image features\n",
    "# num_classes=0 loại bỏ lớp phân loại cuối cùng, trả về feature vector\n",
    "inception_resnet_model = timm.create_model(\"inception_resnet_v2\", pretrained=True, features_only=False)\n",
    "inception_resnet_model.to(DEVICE).eval()\n",
    "print(\">> Model loaded.\")\n",
    "\n",
    "# --- Tải mô hình Mask R-CNN ---\n",
    "print(\"Loading Mask R-CNN model...\")\n",
    "# Tải mô hình Mask R-CNN được huấn luyện sẵn trên COCO\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "maskrcnn_model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "# Chuyển mô hình sang thiết bị và đặt ở chế độ đánh giá\n",
    "maskrcnn_model = maskrcnn_model.to(DEVICE).eval()\n",
    "print(\">> Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for video captioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception-ResNet-V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_image_features(video_frames_bgr):\n",
    "    \"\"\"\n",
    "    Trích xuất đặc trưng hình ảnh từ danh sách khung hình (BGR numpy arrays).\n",
    "    Trả về mảng NumPy có shape (num_frames, feature_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    # Hàm tiền xử lý cho timm/PyTorch\n",
    "    # Lấy thông tin chuẩn hóa từ config của mô hình\n",
    "    data_config = timm.data.resolve_model_data_config(inception_resnet_model)\n",
    "    # Tạo bộ transform chuẩn dựa trên config\n",
    "    # transforms_timm = timm.data.create_transform(**data_config, is_training=False)\n",
    "    transforms_timm = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),    # InceptionResNetV2 expects 299x299\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  # timm models sometimes expect this; if you used different model change accordingly\n",
    "    ])\n",
    "\n",
    "    # Danh sách để lưu đặc trưng\n",
    "    all_image_features = []\n",
    "\n",
    "    print(\"Extracting image features using timm Inception-ResNet-V2...\")\n",
    "    for i, frame_bgr in enumerate(video_frames_bgr):\n",
    "        # In mỗi 10 frames hoặc với i == 0\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            print(f\"  >> Extract image feature for frame {i+1}/{len(video_frames_bgr)}\")\n",
    "\n",
    "        # 1. Chuyển BGR (OpenCV) sang RGB\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB) # numpy array (H, W, C) RGB\n",
    "\n",
    "        # 2. CHUYỂN ĐỔI SANG PIL IMAGE TRƯỚC KHI TIỀN XỬ LÝ\n",
    "        # Chuyển numpy array RGB -> PIL Image\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "\n",
    "        # 3. Tiền xử lý bằng transform của timm\n",
    "        # Giờ truyền PIL Image vào transform\n",
    "        try:\n",
    "            # Áp dụng transform lên PIL Image\n",
    "            input_tensor = transforms_timm(pil_image) # Trả về torch.Tensor (C, H, W)\n",
    "            # Thêm chiều batch\n",
    "            input_batch = input_tensor.unsqueeze(0).to(DEVICE) # (1, C, H, W)\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing frame {i+1}: {e}\")\n",
    "            # Thêm vector zero nếu lỗi\n",
    "            all_image_features.append(np.zeros(1536, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        # 4. Trích xuất đặc trưng (phần còn lại giữ nguyên)\n",
    "        with torch.no_grad():\n",
    "            # features_tensor = inception_resnet_model(input_batch) # (1, 1536)\n",
    "            features_tensor = inception_resnet_model.forward_features(input_batch) # (1, 1536)\n",
    "\n",
    "        if features_tensor.ndim == 4:\n",
    "            features_tensor = torch.flatten(torch.nn.functional.adaptive_avg_pool2d(features_tensor, (1,1)), 1)\n",
    "\n",
    "        # 5. Chuyển tensor PyTorch -> numpy array và loại bỏ chiều batch\n",
    "        features_np = features_tensor.cpu().numpy().squeeze(0) # (1536,)\n",
    "        all_image_features.append(features_np)\n",
    "\n",
    "    # 5. Chuyển đổi danh sách đặc trưng thành mảng NumPy có shape (num_frames, 1536)\n",
    "    if all_image_features:\n",
    "        image_feats = np.array(all_image_features) # Shape: (num_frames, 1536)\n",
    "    else:\n",
    "        print(\"[extract_image_features] No frames processed, returning empty array.\")\n",
    "        image_feats = np.empty((0, 1536), dtype=np.float32)\n",
    "    print(f\"[v] Image feature extraction complete. Shape: {image_feats.shape}\")\n",
    "\n",
    "    return image_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Cấu hình ---\n",
    "TOP_K_PER_FRAME = 20  # Số lượng detection tốt nhất được giữ lại *mỗi khung hình*\n",
    "COORD_MODE = \"cxcywh\"  # Chế độ mã hóa tọa độ bounding box\n",
    "CONFIDENCE_THRESHOLD = 0.7  # Ngưỡng confidence cho detection\n",
    "# Số lượng đặc trưng cuối cùng mong muốn\n",
    "TARGET_TOTAL_FEATURES = 50\n",
    "# --- Kết thúc Cấu hình ---\n",
    "\n",
    "# 4. Hàm tiền xử lý: Chuyển PIL Image -> Tensor (C,H,W), giá trị [0,1]\n",
    "preproc = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "def compute_box_coords(boxes, image_size, mode=\"cxcywh_log\"):\n",
    "    \"\"\"\n",
    "    Tính toán và chuẩn hóa tọa độ bounding box.\n",
    "    \"\"\"\n",
    "    H, W = image_size\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    w = (x2 - x1).clamp(min=1.0)\n",
    "    h = (y2 - y1).clamp(min=1.0)\n",
    "    cx = x1 + 0.5 * w\n",
    "    cy = y1 + 0.5 * h\n",
    "\n",
    "    if mode == \"xyxy_norm\":\n",
    "        coords = torch.stack([x1 / W, y1 / H, x2 / W, y2 / H], dim=1)\n",
    "    elif mode == \"cxcywh\":\n",
    "        coords = torch.stack([cx / W, cy / H, w / W, h / H], dim=1)\n",
    "    else:  # cxcywh_log (mặc định)\n",
    "        coords = torch.stack(\n",
    "            [cx / W, cy / H, torch.log(w / W), torch.log(h / H)], dim=1)\n",
    "    return coords\n",
    "\n",
    "\n",
    "@torch.no_grad()  # Tắt gradient để tăng tốc độ và tiết kiệm bộ nhớ trong suy luận\n",
    "def extract_instance_feats_with_coords(model, pil_img, topk=50, coord_mode=\"cxcywh\", conf_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Trích xuất đặc trưng 1028-D từ các đối tượng trong hình ảnh sử dụng Mask R-CNN.\n",
    "    Args:\n",
    "        model: Mô hình Mask R-CNN đã được tải.\n",
    "        pil_img: PIL Image.\n",
    "        topk: Số lượng detection tốt nhất được giữ lại.\n",
    "        coord_mode: Chế độ mã hóa tọa độ ('cxcywh', 'cxcywh_log', 'xyxy_norm').\n",
    "        conf_threshold: Ngưỡng confidence tối thiểu.\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor đặc trưng có shape (N, 1028).\n",
    "                      N là số lượng đối tượng được phát hiện (<= topk).\n",
    "    \"\"\"\n",
    "    img = pil_img.convert(\"RGB\")\n",
    "\n",
    "    # 1) Chạy mô hình để có detections (boxes, scores, labels, masks...)\n",
    "    # model.forward nhận danh sách tensor\n",
    "    img_tensor = preproc(img).to(DEVICE)  # Chuyển PIL -> Tensor và lên GPU\n",
    "    # Trả về danh sách kết quả cho từng ảnh trong batch\n",
    "    outputs = model([img_tensor])\n",
    "    out = outputs[0]  # Lấy kết quả cho ảnh đầu tiên (batch size = 1)\n",
    "\n",
    "    # Trích xuất boxes, scores, labels\n",
    "    boxes = out.get(\"boxes\", torch.empty(\n",
    "        (0, 4), device=DEVICE))  # Boxes trên device\n",
    "    scores = out.get(\"scores\", torch.empty((0,), device=DEVICE))\n",
    "    labels = out.get(\"labels\", torch.empty(\n",
    "        (0,), device=DEVICE))  # Có thể dùng nếu cần\n",
    "\n",
    "    # 2) Lọc theo ngưỡng confidence\n",
    "    if scores.numel() > 0 and conf_threshold > 0.0:\n",
    "        keep_conf = scores >= conf_threshold\n",
    "        boxes = boxes[keep_conf]\n",
    "        scores = scores[keep_conf]\n",
    "        labels = labels[keep_conf]  # Nếu dùng labels\n",
    "\n",
    "    # 3) Xử lý trường hợp không có detection nào sau khi lọc\n",
    "    if boxes.numel() == 0:\n",
    "        print(\"Warning: No objects detected (or survived filtering). Returning empty features.\")\n",
    "        # Trả về tensor rỗng với shape đúng\n",
    "        return torch.empty((0, 1028), dtype=torch.float32, device='cpu')\n",
    "\n",
    "    # 4) Chọn top-k detections dựa trên score\n",
    "    if scores is not None and scores.numel() > 0:\n",
    "        # torch.topk thường hiệu quả hơn argsort + slice nếu k << total\n",
    "        k = min(topk, boxes.shape[0])\n",
    "        top_scores, order = torch.topk(scores, k, largest=True, sorted=True)\n",
    "        keep = order\n",
    "        boxes = boxes[keep]\n",
    "        scores = scores[keep]\n",
    "        labels = labels[keep]  # Nếu dùng labels\n",
    "    else:\n",
    "        # Nếu không có scores hoặc scores rỗng (hiếm khi xảy ra)\n",
    "        boxes = boxes[:topk]\n",
    "\n",
    "    # --- Bắt đầu trích xuất đặc trưng ---\n",
    "    # 5) Tiền xử lý lại ảnh để đưa vào model.transform\n",
    "    # (model.transform xử lý normalization, resizing nếu cần)\n",
    "    # img_tensor đã được tạo ở trên\n",
    "    # images, _ = model.transform([img_tensor]) # Có thể dùng nếu cần transform lại\n",
    "    # Tuy nhiên, vì img_tensor đã được chuẩn hóa đúng cách bởi preproc và model.transform\n",
    "    # sẽ không làm gì thêm nếu kích thước phù hợp và không có chuyển đổi khác,\n",
    "    # ta có thể bỏ qua bước này và dùng trực tiếp img_tensor.\n",
    "    # Nhưng để đúng logic và chắc chắn, ta vẫn gọi transform.\n",
    "    # img_tensor_for_transform = preproc(img).unsqueeze(0) # Thêm batch dim\n",
    "    images, _ = model.transform([img_tensor])  # Trả về ImageList\n",
    "    images_t = images.tensors.to(DEVICE)    # (1, 3, H', W')\n",
    "    # [(H', W')] - kích thước sau transform\n",
    "    image_sizes = images.image_sizes\n",
    "\n",
    "    # 6) Backbone -> features dict\n",
    "    features = model.backbone(images_t)  # OrderedDict of feature maps\n",
    "\n",
    "    # 7) ROI pooling (boxes phải ở không gian tọa độ ảnh gốc, đã được transform xử lý)\n",
    "    # boxes vẫn đang ở device\n",
    "    pooled = model.roi_heads.box_roi_pool(\n",
    "        features, [boxes], image_sizes)  # (N, 256, 7, 7)\n",
    "\n",
    "    # 8) box head -> (N, 1024)\n",
    "    # Đây là bước quan trọng để lấy feature vector 1024-D\n",
    "    box_repr = model.roi_heads.box_head(pooled)  # (N, 1024)\n",
    "\n",
    "    # 9) Tính toán và chuẩn hóa tọa độ\n",
    "    H_img, W_img = image_sizes[0]  # ints - kích thước ảnh sau khi transform\n",
    "    coords = compute_box_coords(boxes, (H_img, W_img), mode=coord_mode)\n",
    "    coords = coords.to(box_repr.dtype).to(\n",
    "        box_repr.device)  # Đảm bảo dtype và device khớp\n",
    "\n",
    "    # 10) Kết hợp đặc trưng và tọa độ -> (N, 1028)\n",
    "    feat1028 = torch.cat([box_repr, coords], dim=1)  # (N, 1028)\n",
    "\n",
    "    return feat1028.cpu()  # Chuyển về CPU để dễ xử lý sau này\n",
    "\n",
    "\n",
    "def aggregate_features(feature_list, target_num_features):\n",
    "    \"\"\"\n",
    "    Tổng hợp danh sách các tensor đặc trưng thành một tensor cố định.\n",
    "    Args:\n",
    "        feature_list: List of torch.Tensor, each with shape (N_i, 1028).\n",
    "        target_num_features: int, số lượng đặc trưng mong muốn.\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor với shape (target_num_features, 1028).\n",
    "    \"\"\"\n",
    "    if not feature_list:\n",
    "        print(\"Warning: feature_list is empty. Returning zero tensor.\")\n",
    "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
    "\n",
    "    # Gộp tất cả các đặc trưng từ các khung hình\n",
    "    # Bỏ qua các tensor rỗng (0, 1028) nếu có\n",
    "    non_empty_features = [f for f in feature_list if f.shape[0] > 0]\n",
    "\n",
    "    if not non_empty_features:\n",
    "        print(\"Warning: No features detected in any frame. Returning zero tensor.\")\n",
    "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
    "\n",
    "    # Shape: (Total_Detections, 1028)\n",
    "    all_features = torch.cat(non_empty_features, dim=0)\n",
    "    total_detections = all_features.shape[0]\n",
    "    print(\n",
    "        f\"Total object detections across all sampled frames: {total_detections}\")\n",
    "\n",
    "    if total_detections == 0:\n",
    "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
    "\n",
    "    if total_detections >= target_num_features:\n",
    "        # Lấy mẫu đều để giảm xuống target_num_features\n",
    "        indices = np.linspace(0, total_detections - 1,\n",
    "                              target_num_features, dtype=int)\n",
    "        selected_features = all_features[indices]\n",
    "    else:\n",
    "        # Nếu không đủ, pad bằng cách lặp lại đặc trưng cuối cùng\n",
    "        print(\n",
    "            f\"Warning: Only {total_detections} features found. Padding to {target_num_features}.\")\n",
    "        num_to_pad = target_num_features - total_detections\n",
    "        if num_to_pad > 0:\n",
    "            # [num_to_pad, 1028]\n",
    "            padding = all_features[-1:].repeat(num_to_pad, 1)\n",
    "            # [target_num_features, 1028]\n",
    "            selected_features = torch.cat([all_features, padding], dim=0)\n",
    "        else:\n",
    "            selected_features = all_features\n",
    "\n",
    "    # Đảm bảo shape cuối cùng chính xác\n",
    "    assert selected_features.shape == (\n",
    "        target_num_features, 1028), f\"Aggregation failed: {selected_features.shape}\"\n",
    "    return selected_features  # Shape: (target_num_features, 1028)\n",
    "\n",
    "\n",
    "def extract_object_features_from_video(video_frames_bgr):\n",
    "    \"\"\"\n",
    "    Trích xuất đặc trưng đối tượng từ danh sách khung hình video (BGR numpy arrays).\n",
    "    Args:\n",
    "        video_frames_bgr: List of BGR numpy arrays.\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor đặc trưng có shape (1, TARGET_TOTAL_FEATURES, 1028).\n",
    "    \"\"\"\n",
    "\n",
    "    # Tạo placeholder cho relationship features, kích thước (50, 300)\n",
    "    relationship_feats = torch.zeros((TARGET_TOTAL_FEATURES, 300), dtype=torch.float32)\n",
    "\n",
    "    # Danh sách lưu trữ đặc trưng từ từng khung hình\n",
    "    all_frame_features = []\n",
    "\n",
    "    # Lặp qua từng khung hình BGR\n",
    "    print(\"Extracting object features using Mask R-CNN...\")\n",
    "    for i, frame_bgr in enumerate(video_frames_bgr):\n",
    "        # In mỗi 10 frames hoặc với i == 0\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            print(f\"  >> Processing frame {i+1}/{len(video_frames_bgr)}\")\n",
    "        # Chuyển đổi BGR (OpenCV) -> RGB -> PIL Image\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "        # Trích xuất đặc trưng từ khung hình\n",
    "        try:\n",
    "            frame_features = extract_instance_feats_with_coords(\n",
    "                maskrcnn_model, pil_img,\n",
    "                topk=TOP_K_PER_FRAME,\n",
    "                coord_mode=COORD_MODE,\n",
    "                conf_threshold=CONFIDENCE_THRESHOLD\n",
    "            )\n",
    "            # Thêm tensor (N_i, 1028) hoặc (0, 1028)\n",
    "            all_frame_features.append(frame_features)\n",
    "        except Exception as e:\n",
    "            print(f\"    Error extracting features from frame {i+1}: {e}\")\n",
    "            # Thêm tensor rỗng nếu lỗi\n",
    "            all_frame_features.append(\n",
    "                torch.empty((0, 1028), dtype=torch.float32))\n",
    "\n",
    "    # Tổng hợp đặc trưng để có số lượng cố định\n",
    "    print(\"[v] Aggregating features to target shape...\")\n",
    "    object_feats = aggregate_features(\n",
    "        all_frame_features, TARGET_TOTAL_FEATURES)  # Tensor (50, 1028)\n",
    "    print(f\"[v] Object feature extraction complete. Shape: {object_feats.shape}\")\n",
    "\n",
    "    return object_feats, relationship_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- I3D model global để tránh tải lại nhiều lần ---\n",
    "i3d_model = None\n",
    "\n",
    "def extract_motion_features(video_path):\n",
    "    global i3d_model, FEATURE_MODE\n",
    "    if i3d_model is None:\n",
    "        print(\"Loading I3D model...\")\n",
    "        # Select the feature type\n",
    "        feature_type = 'i3d'\n",
    "\n",
    "        # Load and patch the config\n",
    "        args = OmegaConf.load(build_cfg_path(feature_type))\n",
    "        args.video_paths = [video_path]\n",
    "        # args.show_pred = True\n",
    "        # args.stack_size = 64\n",
    "        # args.step_size = 64\n",
    "        # args.extraction_fps = 50\n",
    "        args.flow_type = 'raft'\n",
    "        # args.streams = 'flow'\n",
    "\n",
    "        # Load the model\n",
    "        i3d_model = ExtractI3D(args)\n",
    "        print(\">> I3D Model loaded.\")\n",
    "\n",
    "    # Extract motion features\n",
    "    print(\"Extracting motion features from video...\")\n",
    "    feature_dict = i3d_model.extract(video_path)\n",
    "\n",
    "    # Kết hợp đặc trưng RGB và Flow từ 2 stream\n",
    "    print(\"  >> Combining RGB and Flow features...\")\n",
    "    motion_feats = feature_dict['rgb'] + feature_dict['flow']\n",
    "\n",
    "    # Nếu thời lượng của video quá ngắn, đầu ra có shape (0,)\n",
    "    if motion_feats.shape[0] == 0:\n",
    "        # In thông báo\n",
    "        print(\"[!] Video too short, no motion features extracted. Returning zero tensor.\")\n",
    "        # Trả về tensor zero với shape (1, 1024) để tránh lỗi\n",
    "        motion_feats = np.zeros((1, 1024), dtype=np.float32)\n",
    "        # # Cập nhật lại FEATURE_MODE để tránh lỗi trong mô hình\n",
    "        # FEATURE_MODE = \"two\"\n",
    "        # print(f\"  >> Updated FEATURE_MODE to '{FEATURE_MODE}' to handle short video.\")\n",
    "\n",
    "    print(f\"[v] Motion feature extraction complete. Shape: {motion_feats.shape}\")\n",
    "\n",
    "    return motion_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline for feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_frames(video_path, target_fps):\n",
    "    \"\"\" Hàm lấy mẫu khung hình từ video ở fps mục tiêu.\n",
    "    Ảnh trả về là danh sách các mảng numpy (BGR).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    interval = max(1, int(round(orig_fps / target_fps)))\n",
    "    frames = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx % interval == 0:\n",
    "            # Giữ nguyên ở dạng BGR hoặc chuyển sang RGB nếu mô hình yêu cầu (InceptionResNetV2 dùng RGB)\n",
    "            # frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) # Nếu cần RGB\n",
    "            frames.append(frame)  # Giữ BGR, sẽ xử lý sau\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def resample_fixed(feats, N):\n",
    "    \"\"\" Hàm lấy mẫu lại chuỗi đặc trưng để có đúng N đặc trưng bằng cách lấy mẫu đều.\n",
    "    \"\"\"\n",
    "    T, D = feats.shape\n",
    "    idxs = np.linspace(0, T-1, N).astype(int)\n",
    "    return feats[idxs]\n",
    "\n",
    "\n",
    "def process_video(video_path, dataset='MSVD'):\n",
    "    \"\"\" Full pipeline để xử lý video và trích xuất đặc trưng.\n",
    "    \"\"\"\n",
    "    N = 50 if dataset == 'MSVD' else 60\n",
    "    target_fps = 5 if dataset == 'MSVD' else 3\n",
    "\n",
    "    # --- Tải video và lấy mẫu frames ---\n",
    "    print(\"Sampling video frames...\")\n",
    "    # Danh sách các mảng numpy (BGR)\n",
    "    video_frames = sample_frames(video_path, target_fps)\n",
    "    print(f\"Sampled {len(video_frames)} frames at ~{target_fps} fps.\")\n",
    "\n",
    "    # Nếu có nhiều hơn 50 frames, lấy 50 frames cách đều nhau để giảm bớt\n",
    "    if len(video_frames) > 50:\n",
    "        indices = np.linspace(0, len(video_frames) - 1, 50, dtype=int)\n",
    "        video_frames = [video_frames[i] for i in indices]\n",
    "        print(f\"Reduced to {len(video_frames)} frames for processing.\")\n",
    "\n",
    "    # Trích xuất đặc trưng với mô hình Inception-ResNet-V2\n",
    "    image_feats = extract_image_features(video_frames)\n",
    "    # Trích xuất đặc trưng với mô hình Mask R-CNN\n",
    "    object_feats, rel_feats = extract_object_features_from_video(video_frames)\n",
    "    # Trích xuất đặc trưng với mô hình I3D\n",
    "    motion_feats = extract_motion_features(video_path)\n",
    "\n",
    "    # Resample để có đúng N đặc trưng\n",
    "    image_feats_res = resample_fixed(image_feats, N)  # (N, 1536)\n",
    "    object_feats_res = resample_fixed(object_feats.numpy(), N)  # (N, 1028)\n",
    "    rel_feats_res = resample_fixed(rel_feats.numpy(), N)  # (N, 300)\n",
    "    motion_feats_res = resample_fixed(motion_feats, N)  # (N, 1024)\n",
    "\n",
    "    # Đảm bảo tất cả đặc trưng đều có kiểu dữ liệu float32\n",
    "    image_feats_res = image_feats_res.astype(np.float32)\n",
    "    object_feats_res = object_feats_res.astype(np.float32)\n",
    "    rel_feats_res = rel_feats_res.astype(np.float32)\n",
    "    motion_feats_res = motion_feats_res.astype(np.float32)\n",
    "\n",
    "    # Chuyển sang tensor và thêm batch dim\n",
    "    image_feats_tensor = torch.from_numpy(image_feats_res).to(\n",
    "        DEVICE).unsqueeze(0)  # (1, N, 1536)\n",
    "    object_feats_tensor = torch.from_numpy(object_feats_res).to(\n",
    "        DEVICE).unsqueeze(0)  # (1, N, 1028)\n",
    "    rel_feats_tensor = torch.from_numpy(rel_feats_res).to(\n",
    "        DEVICE).unsqueeze(0)  # (1, N, 300)\n",
    "    motion_feats_tensor = torch.from_numpy(motion_feats_res).to(\n",
    "        DEVICE).unsqueeze(0)  # (1, N, 1024)\n",
    "\n",
    "    # Trả về kết quả\n",
    "    return {\n",
    "        'image_feats': image_feats_tensor,\n",
    "        'motion_feats': motion_feats_tensor,\n",
    "        'object_feats': object_feats_tensor,\n",
    "        'rel_feats': rel_feats_tensor\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint and config for BTKG model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_path +  \"/checkpoints/best.ckpt\", map_location=\"cpu\")\n",
    "config = dict_to_cls(checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def change_src_dir(src, dst):\n",
    "    if os.path.islink(dst) or os.path.isdir(dst):\n",
    "        os.unlink(dst)\n",
    "    # Now create the symlink\n",
    "    os.symlink(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src = folder_path + '/models'\n",
    "dst = '/kaggle/working/models'\n",
    "change_src_dir(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src = folder_path + '/data'\n",
    "dst = '/kaggle/working/data'\n",
    "change_src_dir(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src = folder_path + \"/video_feature_configs\"\n",
    "dst = '/kaggle/working/video_feature_configs'\n",
    "change_src_dir(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove if already exists and is a symlink or directory\n",
    "corpus = MSVD(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BTKG model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab = corpus.vocab\n",
    "\"\"\" Build Models \"\"\"\n",
    "try:\n",
    "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
    "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
    "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big,\n",
    "                           select_num=config.transformer.select_num)\n",
    "except:\n",
    "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
    "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
    "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big)\n",
    "model.load_state_dict(checkpoint['abd_transformer'])\n",
    "model.device = DEVICE\n",
    "model.feature_mode = FEATURE_MODE\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_video_motion_caption(video_path):\n",
    "    global FEATURE_MODE\n",
    "    # Reset feature mode in case it was changed\n",
    "    FEATURE_MODE = \"three\"\n",
    "\n",
    "    # Rút trích đặc trưng từ video\n",
    "    feats_dict = process_video(video_path)\n",
    "    feats = (\n",
    "        feats_dict['image_feats'],\n",
    "        feats_dict['motion_feats'],\n",
    "        feats_dict['object_feats'],\n",
    "        feats_dict['rel_feats']\n",
    "    )\n",
    "    # Tạo caption cho video\n",
    "    model.eval()\n",
    "    model.feature_mode = FEATURE_MODE\n",
    "    print(f\"Generating caption with feature mode: {model.feature_mode}\")\n",
    "    beam_size = config.beam_size\n",
    "    max_len = config.loader.max_caption_len\n",
    "    with torch.no_grad():\n",
    "        r2l_captions, l2r_captions = model.beam_search_decode(feats, beam_size, max_len)\n",
    "        # r2l_captions = [idxs_to_sentence(caption, vocab.idx2word, BOS_idx) for caption in r2l_captions]\n",
    "        l2r_captions = [\" \".join(caption[0].value) for caption in l2r_captions]\n",
    "        r2l_captions = [\" \".join(caption[0].value) for caption in r2l_captions]\n",
    "\n",
    "    print(f\"Left to Right Captions: {l2r_captions}\")\n",
    "    return l2r_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Duyệt qua từng video trong folder 'videos' và tạo caption\n",
    "# import os\n",
    "# for filename in os.listdir('/kaggle/input/video-captiong-video'):\n",
    "#     video_path = os.path.join('/kaggle/input/video-captiong-video', filename)\n",
    "#     print(f\"Generating caption for {filename}...\")\n",
    "#     captions = generate_video_motion_caption(video_path)\n",
    "#     print(f\">> Captions for {filename}: {captions}\\n\")\n",
    "#     print(\"--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_video_path(file: UploadFile = File(...)):\n",
    "    # Save uploaded video to disk\n",
    "    suffix = Path(file.filename or \"video\").suffix or \".mp4\"\n",
    "    temp_path = UPLOAD_DIR / f\"{uuid.uuid4().hex}{suffix}\"\n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        shutil.copyfileobj(file.file, f)\n",
    "    video_path = str(temp_path)\n",
    "    return video_path\n",
    "\n",
    "def get_time(s: int) -> str:\n",
    "    m: int = s // 60   # use integer division instead of /\n",
    "    s = s % 60\n",
    "    return f\"{m:02d}m {s:02d}s\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOhjsFQ-zLKe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "origins = [\n",
    "    \"http://localhost:3000\",\n",
    "    \"https://vidcap.vercel.app\"\n",
    "]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "ngrok.set_auth_token(\"30x6yoyG84W4rwy5Hcp0yso2uWS_6kVDCjkNCbhTE56MUs7q9\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"FastAPI Video Captioning API is running!\"}\n",
    "    \n",
    "WORKDIR = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else Path.cwd()\n",
    "UPLOAD_DIR = WORKDIR / \"uploads\"\n",
    "UPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "    \n",
    "@app.post(\"/upload\")\n",
    "async def get_uuid(file: UploadFile = File(...)):\n",
    "    return JSONResponse(\n",
    "        {\n",
    "            \"ok\": True,\n",
    "            \"path\": get_video_path(file)\n",
    "            # \"transcript\": all_transcript\n",
    "        }\n",
    "    )\n",
    "    \n",
    "# @app.delete(\"delete\")\n",
    "# asyn def delete_uuid()\n",
    "\n",
    "@app.post(\"/image_process\")\n",
    "async def image_process(video_path):\n",
    "    \"\"\"\n",
    "    Accepts an uploaded video file, runs keyframe extraction, and returns the output paths.\n",
    "    \"\"\"\n",
    "\n",
    "    directory_to_zip, output_zip_path_no_ext = extract(video_path)\n",
    "\n",
    "    frames_dir = directory_to_zip\n",
    "    zip_file = f\"{output_zip_path_no_ext}.zip\"\n",
    "    image_list = [\n",
    "        os.path.join(frames_dir, f)\n",
    "        for f in os.listdir(frames_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\"))\n",
    "    ]\n",
    "    files_sorted = sorted(\n",
    "        image_list,\n",
    "        key=lambda x: int(x.split(\"keyframe_\")[1].split(\"_\")[0])\n",
    "    )\n",
    "    \n",
    "\n",
    "    ic_list = []\n",
    "    od_list = []\n",
    "    for i in files_sorted:\n",
    "        splited_str = i.split(\"keyframe_\")[1].split(\"_\")\n",
    "        m = splited_str[2]\n",
    "        s =  splited_str[3]\n",
    "        ms =  splited_str[4].split('.')[0]\n",
    "        prefix_str = m + 'm ' + s + 's ' + ms + 'ms:' \n",
    "        ic_list.append(prefix_str + generate_caption(i))\n",
    "        od_list.append(prefix_str + yolo_extract(i))\n",
    "    all_image_captioning = \"\\n\".join(ic_list)   \n",
    "    all_object_detection = \"\\n\".join(od_list)\n",
    "    \n",
    "    \n",
    "    return JSONResponse(\n",
    "        {\n",
    "            \"ok\": True,\n",
    "            \"frames_dir\": frames_dir,\n",
    "            \"zip_path\": zip_file,\n",
    "            \"ic\": all_image_captioning,\n",
    "            \"od\": all_object_detection\n",
    "            \n",
    "        }\n",
    "    )\n",
    "\n",
    "@app.post(\"/video_process/\")\n",
    "async def video_process(video_path, chunk_sec:int = 20):\n",
    "    # Save uploaded video to disk\n",
    "    # video_path = get_video_path(file)\n",
    "    out_audio = split_media_ffmpeg(\n",
    "        video_path,\n",
    "        chunk_sec=chunk_sec,\n",
    "        out_dir=\"/kaggle/working/my_audio_chunks\",\n",
    "        prefix=\"my_audio\",\n",
    "        copy_streams=True\n",
    "    )\n",
    "    transcrips = []\n",
    "    vm_list = []\n",
    "    s = 0\n",
    "    for v in out_audio:\n",
    "        t = get_time(s)\n",
    "        transcrips.append(t + \": \" + get_transcript(v))\n",
    "        print(t,' transcrips done')\n",
    "        vm_list.append(t + \": \" + generate_video_motion_caption(v)[0])\n",
    "        print(t,' vm done' )\n",
    "        s += chunk_sec\n",
    "        print(v)\n",
    "    all_vm = \"\\n\".join(vm_list)\n",
    "    all_transcript = \"\\n\".join(transcrips)\n",
    "    \n",
    "    return JSONResponse(\n",
    "        {\n",
    "            \"ok\": True,\n",
    "            \"video_motion\": all_vm,\n",
    "            \"transcript\": all_transcript\n",
    "        }\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Kill previous ngrok tunnels if any\n",
    "ngrok.kill()\n",
    "nest_asyncio.apply()\n",
    "# Open a tunnel on port 8000\n",
    "public_url = ngrok.connect(8000)\n",
    "url = \"https://vidcap.vercel.app/api/caption\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "public_url = public_url.public_url\n",
    "print(f\"Public URL: {public_url}\")\n",
    "data = {\n",
    "    'url': public_url\n",
    "}\n",
    "response = requests.post(url, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Force log to stdout\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "def run_api():\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        access_log=True,\n",
    "        use_colors=True\n",
    "    )\n",
    "\n",
    "thread = threading.Thread(target=run_api, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8041957,
     "sourceId": 12723512,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7814133,
     "sourceId": 12798241,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 426658,
     "modelInstanceId": 408796,
     "sourceId": 519180,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 421970,
     "modelInstanceId": 404051,
     "sourceId": 536336,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
