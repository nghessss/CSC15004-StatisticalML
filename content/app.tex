\section{Applications}

We investigated the applications of our model across various domains. The model's ability to generate coherent and contextually relevant text makes it suitable for question-answering tasks with a chatbot.



\subsection{Chatbot Application}

We used Gemini 2.5 Flash~\cite{comanici2025gemini} to create a chatbot that can answer questions based on the provided context. The chatbot is designed to understand and respond to user queries in a conversational manner, leveraging the model's capabilities to generate human-like responses.

When a user enters a question accompanied by a short video, the video goes through our two-step captioning process--video captioning and keyframe captioning--and the chatbot then answers the question based on the generated captions. The chatbot is capable of understanding the context of the video and providing relevant answers to user queries.

\subsection{Enhancing Context with Object Detection}

While our captioning models provide high-level summaries of scenes and actions, they may not always capture specific details about every object present in the video. A caption might say, ``\textit{A person is working in an office}'', but it might not list the specific items on the desk, such as a laptop, a mug, or a phone.

To create a more detailed and robust context for our chatbot, we integrated an \textbf{object detection} model. For this task, we chose the \textbf{Ultralytics YOLOv11}~\cite{yolo11_ultralytics} model, using a version pre-trained on the comprehensive \textbf{COCO dataset}. This model is highly efficient and accurate at identifying a wide range of common objects.

The integration process is designed to complement our existing workflow. The same keyframes that are sent to the image captioning model are also processed by YOLOv11. The output from YOLO is a list of all detected objects within those frames. This list is then added to the overall context that is fed to the chatbot.


\subsection{Enhancing Context with Speech Recognition}

To capture the auditory information within the video, we integrated an automatic speech recognition (ASR) module. For this purpose, we selected PhoWhisper~\cite{le2024phowhisper}, a state-of-the-art model specifically fine-tuned for the Vietnamese language. Based on OpenAI's robust Whisper architecture, PhoWhisper is trained on a large and diverse dataset of Vietnamese accents, ensuring high accuracy for transcription.

The integration process is straightforward: the audio stream is first extracted from the input video and then processed by the PhoWhisper model. The output is a complete text transcript of any spoken dialogue. This transcript is then added to the overall context fed to the chatbot, allowing it to understand and answer questions based on what is said in the video.


\subsection{User Interface}

% We designed a user-friendly interface for the chatbot, allowing users to interact with it easily. The interface includes a video player for watching the video, a text input box for entering questions, and a chat window for displaying the chatbot's responses. This setup ensures a seamless user experience, enabling users to engage with the chatbot while watching the video.