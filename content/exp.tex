\section{Experiments}

\subsection{Experimental Settings}

\subsubsection{Datasets}

The MSVD dataset (Microsoft Research Video Description) was used as the evaluation benchmark. MSVD contains 1,970 short YouTube video clips (10-25 seconds each) with roughly 80,000 English sentences (about 40 captions per clip). These videos are split following standard practice: 1,200 clips for training, 100 for validation, and 670 for testing. (All captions are in English and describe a single-activity scene per clip, as in prior work.)

\subsubsection{Feature extraction}

We extract both spatial and motion features from the video clips. Video frames were sampled at 5 frames per second (fps) and fed into a pretrained Inception-ResNet-V2~\cite{szegedy2016inceptionv4} network to obtain image features. Concurrently, videos were resampled at 25 fps and divided into overlapping 64-frame segments (stride of 5 frames); these segments were processed by an Inflated 3D ConvNet (I3D)~\cite{Carreira_2017_CVPR} to compute motion features. The resulting image and motion feature vectors have dimensions \textit{2048} and \textit{1024}, respectively. We also encode each video's category label using a 300-dimensional GloVe word vector. From each video's features, 50 frames are uniformly sampled, and the corresponding image, motion, and category feature vectors are then projected into a shared 512-dimensional space.

\subsubsection{Evaluation metrics}

Performance on MSVD was measured with standard automatic captioning metrics: BLEU~\cite{papineni-etal-2002-bleu}, METEOR~\cite{banerjee-lavie-2005-meteor}, CIDEr-D~\cite{vedantam2015cider}, and ROUGE-L~\cite{lin-2004-rouge}. BLEU and METEOR (originally developed for machine translation) quantify the quality of generated text by n-gram overlap with reference captions. CIDEr-D, explicitly designed for caption evaluation, measures consensus with human annotations. ROUGE-L (longest-common-subsequence based) was also used; it emphasizes recall in overlap between generated and reference sentences. These metrics together provide a comprehensive assessment of caption accuracy, relevance, and fluency.

\subsubsection{Parameter settings}

We detail the model's settings for MSVD experiments. The object detector Mask R-CNN~\cite{he2017mask} used a confidence threshold of 0.7 and minimum box size of 224x224. A TransE knowledge-graph was built from 613 unique triples, with each relationship encoded by a 300-dimensional GloVe vector~\cite{pennington-etal-2014-glove}. The Transformer encoders and decoders use 4 layers, with 640-dimensional input embeddings, 512-dimensional model hidden states, 2048-dimensional feedforward layers, and 10 attention heads per layer. (An extended multi-head attention in the STE component uses 128 heads.) During training on MSVD, the balance weight $\lambda$ between forward/backward decoding was set to 0.6 and the model was trained for 30 epochs. We used the Adam optimizer with a batch size of 32 and a learning rate of 1e-4 for MSVD training. Inference used beam search of size 5 with dropout 0.1. All experiments were run on NVIDIA Tesla P100 GPUs.
