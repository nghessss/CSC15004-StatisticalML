\section{Experiments}

\subsection{Experimental Settings}

\subsubsection{Datasets}

The MSVD dataset (Microsoft Research Video Description) was used as the evaluation benchmark for BTKG model. MSVD contains 1,970 short YouTube video clips (10-25 seconds each) with roughly 80,000 English sentences (about 40 captions per clip). These videos are split following standard practice: 1,200 clips for training, 100 for validation, and 670 for testing. (All captions are in English and describe a single-activity scene per clip, as in prior work.)

For the image captioning model, we used the COCO-2017 dataset with Vietnamese captions~\cite{vietnamese_coco_2017}, and for Vietnamese video captioning we used the KTVIC dataset~\cite{pham2024ktvic}. The training split contains 619{,}723 captions aligned with 123{,}879 images (appropriate 5 captions per image). The validation split has 3{,}177 images with one caption each (3{,}177 captions), and the test split has 558 images with one caption each (selected from the original KTVIC test list).

\subsubsection{Feature extraction (BTKG)}

We extract both spatial and motion features from the video clips. Video frames were sampled at 5 frames per second (fps) and fed into a pretrained Inception-ResNet-V2~\cite{szegedy2016inceptionv4} network to obtain image features. Concurrently, videos were resampled at 25 fps and divided into overlapping 64-frame segments (stride of 5 frames); these segments were processed by an Inflated 3D ConvNet (I3D)~\cite{Carreira_2017_CVPR} to compute motion features. The resulting image and motion feature vectors have dimensions \textit{2048} and \textit{1024}, respectively. We also encode each video's category label using a 300-dimensional GloVe word vector. From each video's features, 50 frames are uniformly sampled, and the corresponding image, motion, and category feature vectors are then projected into a shared 512-dimensional space.

\subsubsection{Evaluation metrics}

% Performance on MSVD was measured with standard automatic captioning metrics: BLEU~\cite{papineni-etal-2002-bleu}, METEOR~\cite{banerjee-lavie-2005-meteor}, CIDEr-D~\cite{vedantam2015cider}, and ROUGE-L~\cite{lin-2004-rouge}. BLEU and METEOR (originally developed for machine translation) quantify the quality of generated text by n-gram overlap with reference captions. CIDEr-D, explicitly designed for caption evaluation, measures consensus with human annotations. ROUGE-L (longest-common-subsequence based) was also used; it emphasizes recall in overlap between generated and reference sentences. These metrics together provide a comprehensive assessment of caption accuracy, relevance, and fluency.

We evaluate both tasks--(i) video captioning on MSVD and (ii) keyframe-based image captioning on the Vietnamese datasets--using the same suite of automatic metrics: BLEU~\cite{papineni-etal-2002-bleu}, METEOR~\cite{banerjee-lavie-2005-meteor}, CIDEr\mbox{-}D~\cite{vedantam2015cider}, and ROUGE\mbox{-}L~\cite{lin-2004-rouge}. 
BLEU and METEOR (originating from machine translation) quantify n\mbox{-}gram overlap between generated and reference captions. 
CIDEr\mbox{-}D, designed for caption evaluation, measures consensus with human annotations, while ROUGE\mbox{-}L (based on longest common subsequence) emphasizes recall. 
Unless otherwise noted, we report corpus\mbox{-}level scores; BLEU refers to BLEU@4, CIDEr to the CIDEr\mbox{-}D variant, and ROUGE to ROUGE\mbox{-}L.


\subsubsection{Parameter settings}

% We detail the model's settings for MSVD experiments. The object detector Mask R-CNN~\cite{he2017mask} used a confidence threshold of 0.7 and minimum box size of 224x224. A TransE knowledge-graph was built from 613 unique triples, with each relationship encoded by a 300-dimensional GloVe vector~\cite{pennington-etal-2014-glove}. The Transformer encoders and decoders use 4 layers, with 640-dimensional input embeddings, 512-dimensional model hidden states, 2048-dimensional feedforward layers, and 10 attention heads per layer. (An extended multi-head attention in the STE component uses 128 heads.) During training on MSVD, the balance weight $\lambda$ between forward/backward decoding was set to 0.6 and the model was trained for 30 epochs. We used the Adam optimizer with a batch size of 32 and a learning rate of 1e-4 for MSVD training. Inference used beam search of size 5 with dropout 0.1. All experiments were run on NVIDIA Tesla P100 GPUs.

We detail the model's settings for MSVD experiments. The object detector Mask R-CNN~\cite{he2017mask} used a confidence threshold of 0.7 and minimum box size of \(224\times224\). A TransE knowledge-graph was built from 613 unique triples, with each relationship encoded by a 300-dimensional GloVe vector~\cite{pennington-etal-2014-glove}. The Transformer encoders and decoders use 4 layers, with 640-dimensional input embeddings, 512-dimensional model hidden states, 2048-dimensional feedforward layers, and 10 attention heads per layer. (An extended multi-head attention in the STE component uses 128 heads.) During training on MSVD, the balance weight $\lambda$ between forward/backward decoding was set to 0.6 and the model was trained for 30 epochs. We used the Adam optimizer with a batch size of 32 and a learning rate of \(1\mathrm{e}{-4}\) for MSVD training. Inference used beam search of size 5 with dropout 0.1. 

For the image captioning model, we train a ViT\textendash T5 captioner that uses a ViT-Base/16 vision encoder (vit\_base\_patch16\_224) and a Vietnamese T5 decoder (VietAI/vit5-base). Images are resized to \(224\times224\) and normalized with ImageNet mean/std; a linear layer projects ViT patch embeddings to the T5 \(d_{\mathrm{model}}\), whose encoder hidden states are replaced by the projected visual features. Captions are tokenized with the T5 tokenizer and generated with a maximum length of 35 tokens. Training uses AdamW (learning rate \(5\mathrm{e}{-5}\), weight decay \(1\mathrm{e}{-4}\)), batch size 32, cosine-annealing with warm restarts (\(T_0{=}1000\), \(\eta_{\min}{=}1\mathrm{e}{-7}\)), and early stopping with patience 5; we train for 8 epochs. Data loading uses pinned memory and \texttt{num\_workers} equal to the machine's CPU cores. Inference uses beam search with beam size 3 and early stopping. 

All experiments were run on NVIDIA Tesla P100 GPUs.


\subsection{Results and Discussion}

\subsubsection{BTKG Model}

We evaluated our proposed enhancements on the MSVD dataset. The table below presents a comparison between the results published in the original paper, our re-implemented baseline, and our enhanced models.

\begin{table}[H]
  \centering
  \renewcommand{\arraystretch}{1.5}
  \normalsize
  \label{tab:model_comparison}
  \begin{tabular}{lcccc}
    \hline
    \textbf{Model Version} & \textbf{BLEU@4} &
    \textbf{METEOR} & \textbf{ROUGE-L} & \textbf{CIDEr-D} \\
    \hline
    \textbf{Published Results} & 55.7 & 38.3 & 74.7 & 104.5   \\
    \hline
    BTKG (Our Baseline) & 54.6 & 37.9 & 74.3 & 96.8    \\
    BTKG + Feature Fusion & 55.1 & 38.0 & 74.1 & 98.1    \\
    BTKG + Feature Fusion + ResiDual & 55.5   & 38.3   & 74.9    & 100.2   \\
    \hline
  \end{tabular}
  \caption{Performance comparison of BTKG models on MSVD dataset.}
\end{table}

\paragraph{Comparison with Published Baseline:} Our re-implementation of the original BTKG model yielded slightly lower scores than those reported in the paper, most notably a drop in the CIDEr-D score from 104.5 to 96.8. This discrepancy can be attributed to differences in the experimental environment (the original paper used an RTX 3060 GPU, whereas our experiments were run on an NVIDIA Tesla P100 on Kaggle) or minor variations in data preprocessing. Nonetheless, this result serves as a solid baseline for evaluating our proposed improvements.

\paragraph{Effectiveness of the Feature Fusion Module:} Replacing element-wise addition with our learnable \textbf{Feature Fusion} module led to a noticeable improvement across most metrics, especially CIDEr-D, which increased from 96.8 to 98.1. This confirms that a learnable, weighted combination of features is more effective than a simple, static summation. The model can better prioritize more relevant information, resulting in a richer fused feature vector for the decoder.

\paragraph{Positive Impact of the ResiDual Architecture:} The most significant improvement came from integrating the \textbf{ResiDual} architecture. The ``\textbf{BTKG + Feature Fusion + ResiDual}'' model substantially outperformed our baseline across all metrics. Notably, the CIDEr-D score saw a strong increase to 100.2, and the METEOR score reached 38.3, matching the published result. By stabilizing the training process and preventing representation collapse, ResiDual allows the deeper layers of the model to learn more complex and diverse representations. This directly enhances the model's ability to capture the fine-grained semantics of the video, leading to more accurate and semantically appropriate captions, as reflected by the significant gain in the CIDEr-D score.

\subsubsection*{Observations}

While our best-performing model has not yet surpassed all the scores from the original publication, our experimental results clearly demonstrate that:

\begin{enumerate}[nosep]
    \item Using a learnable mechanism like \textbf{Feature Fusion} is more effective for combining multimodal features than direct summation.
    
    \item Upgrading the Transformer architecture to a more stable and powerful design like \textbf{ResiDual} yields significant performance gains, particularly in the semantic quality of the generated captions.
\end{enumerate}

These findings validate our research direction and show that the proposed enhancements have great potential for improving the overall effectiveness of the BTKG model for video captioning.


\subsubsection{ViT-T5 Image Captioning Model}

We also evaluated the ViT-T5 image captioning model on the KTVIC dataset, which contains images with Vietnamese captions.

\begin{table}[H]
  \centering
  \renewcommand{\arraystretch}{1.5}
  \normalsize
  \label{tab:ktvic_comparison}
  \begin{tabular}{lcccc}
    \hline
    \textbf{Encoder - Decoder} & \textbf{BLEU@4} &
    \textbf{METEOR} & \textbf{ROUGE-L} & \textbf{CIDEr-D} \\
    \hline
    ResNet101 - LSTM & 15.5 & 27.5 & 42.3 & 21.8   \\
    ViT - Transformer & 19.3 & 30.1 & 44.7 & 36.8    \\
    GRIT - GRIT & \textbf{40.6} & 36.6 & \textbf{59.7} & \textbf{136.0}    \\
    \hline
    ViT - T5 (Ours) & 30.3 & \textbf{48.8} & 57.3 & 88.3 \\
    \hline
  \end{tabular}
  \caption{Performance of our ViT-T5 model on the KTVIC dataset. Results for ResNet101-LSTM, ViT-Transformer, and GRIT-GRIT are reported from the original KTVIC dataset paper~\cite{pham2024ktvic} for reference.}
\end{table}

These results highlight an effective trade-off between performance and architectural simplicity. While the more complex GRIT model sets a high benchmark, our ViT-T5 model, which is based on a standard Transformer architecture appropriate for the scope of this course, proves to be highly effective. It not only surpasses the ResNet101-LSTM and ViT-Transformer baselines on all metrics but also achieves the highest METEOR score, indicating its particular strength in capturing semantic similarity and fluency.