{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca90194",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e8b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import TrainConfig as C\n",
    "from models.abd_transformer import ABDTransformer\n",
    "import torch\n",
    "from utils import dict_to_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331fd7c",
   "metadata": {},
   "source": [
    "# Load checkpoint and config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoints/best.ckpt\", map_location=\"cpu\")\n",
    "config = dict_to_cls(checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d47827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader.MSVD import MSVD\n",
    "corpus = MSVD(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d87b1",
   "metadata": {},
   "source": [
    "# Build Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f8ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = corpus.vocab\n",
    "\"\"\" Build Models \"\"\"\n",
    "try:\n",
    "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
    "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
    "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big,\n",
    "                           select_num=config.transformer.select_num)\n",
    "except:\n",
    "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
    "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
    "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big)\n",
    "model.load_state_dict(checkpoint['abd_transformer'])\n",
    "model.device = \"cpu\"\n",
    "\n",
    "# Move model to cpu\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0eed1",
   "metadata": {},
   "source": [
    "# Load extracted features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features shape: torch.Size([1, 50, 1536])\n",
      "Motion features shape: torch.Size([1, 50, 1024])\n",
      "Object features shape: torch.Size([1, 50, 1028])\n",
      "Relation features shape: torch.Size([1, 50, 300])\n"
     ]
    }
   ],
   "source": [
    "# Load saved features\n",
    "image_feats = torch.load('features/image_feats.pt', map_location=\"cpu\")\n",
    "motion_feats = torch.load('features/motion_feats.pt', map_location=\"cpu\")\n",
    "obect_feats = torch.load('features/object_feats.pt', map_location=\"cpu\")\n",
    "rel_feats = torch.load('features/rel_feats.pt', map_location=\"cpu\")\n",
    "\n",
    "# # Reshape to 3D tensor, the first dimension is 1\n",
    "# image_feats = image_feats.unsqueeze(0)\n",
    "# motion_feats = motion_feats.unsqueeze(0)[:,:,::2]\n",
    "# obect_feats = obect_feats.unsqueeze(0)\n",
    "# rel_feats = rel_feats.unsqueeze(0)\n",
    "\n",
    "print(\"Image features shape:\", image_feats.shape)\n",
    "print(\"Motion features shape:\", motion_feats.shape)\n",
    "print(\"Object features shape:\", obect_feats.shape)\n",
    "print(\"Relation features shape:\", rel_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af2738",
   "metadata": {},
   "source": [
    "# Inference with beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3d06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left to Right Captions: ['the person is doing the something']\n",
      "CPU times: user 6.62 s, sys: 0 ns, total: 6.62 s\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "beam_size = config.beam_size\n",
    "max_len = config.loader.max_caption_len\n",
    "feature_mode = config.feat.feature_mode\n",
    "feats = (image_feats, motion_feats, obect_feats, rel_feats)\n",
    "with torch.no_grad():\n",
    "    r2l_captions, l2r_captions = model.beam_search_decode(feats, beam_size, max_len)\n",
    "    # r2l_captions = [idxs_to_sentence(caption, vocab.idx2word, BOS_idx) for caption in r2l_captions]\n",
    "    l2r_captions = [\" \".join(caption[0].value) for caption in l2r_captions]\n",
    "    r2l_captions = [\" \".join(caption[0].value) for caption in r2l_captions]\n",
    "    \n",
    "    print(f\"Left to Right Captions: {l2r_captions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
