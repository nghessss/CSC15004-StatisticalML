{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ou0YZkMt0K6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou0YZkMt0K6b",
        "outputId": "f8212a0b-71f1-4fb4-a426-d01a689125eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5dl2kh820epq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dl2kh820epq",
        "outputId": "d3bc1d90-19d6-45f5-e083-d53dfad5390c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/btkg\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/btkg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c66MfBmX0lM6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66MfBmX0lM6",
        "outputId": "d666b7ff-bc0b-4fa6-bdd7-95ae0bc94540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints   models\t     test_inference.ipynb   video_feature_utils\n",
            "config.py     __pycache__    tmp\t\t    videos\n",
            "data\t      pycocoevalcap  train.py\n",
            "inference.py  run.py\t     utils.py\n",
            "loader\t      splits\t     video_feature_configs\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ca90194",
      "metadata": {
        "id": "1ca90194"
      },
      "source": [
        "# Import necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "70e8b632",
      "metadata": {
        "id": "70e8b632"
      },
      "outputs": [],
      "source": [
        "# For video captioning\n",
        "from loader.MSVD import MSVD\n",
        "from config import TrainConfig as C\n",
        "from models.abd_transformer import ABDTransformer\n",
        "import torch\n",
        "from utils import dict_to_cls\n",
        "# Inception-ResNet-V2 for image feature extraction\n",
        "import os\n",
        "import cv2 # Thêm cv2 để đọc video\n",
        "import numpy as np # Thêm numpy để xử lý mảng\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n",
        "# I3D for motion feature extraction\n",
        "from models.i3d.extract_i3d import ExtractI3D\n",
        "from video_feature_utils.utils import build_cfg_path\n",
        "from omegaconf import OmegaConf\n",
        "# Mask R-CNN for object feature extraction\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987dc0b9",
      "metadata": {
        "id": "987dc0b9"
      },
      "source": [
        "# Thiết lập chung\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d89d9959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d89d9959",
        "outputId": "9c0b8519-d3ad-49c1-b819-1824f1fc2bc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# 1) Thiết lập chung\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "887d81ea",
      "metadata": {
        "id": "887d81ea"
      },
      "source": [
        "# Load pretrained model for feature extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b8ef5636",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ef5636",
        "outputId": "b52566be-816f-4f48-c992-29299e789a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Inception-ResNet-V2 model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            ">> Model loaded.\n",
            "Loading Mask R-CNN model...\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:00<00:00, 187MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Model loaded.\n"
          ]
        }
      ],
      "source": [
        "# --- Tải mô hình Inception-ResNet-V2 ---\n",
        "print(\"Loading Inception-ResNet-V2 model...\")\n",
        "# include_top=False để lấy đặc trưng, pooling='avg' để ra vector 1D\n",
        "inception_resnet_model = InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
        "print(\">> Model loaded.\")\n",
        "\n",
        "# --- Tải mô hình Mask R-CNN ---\n",
        "print(\"Loading Mask R-CNN model...\")\n",
        "# Tải mô hình Mask R-CNN được huấn luyện sẵn trên COCO\n",
        "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "maskrcnn_model = maskrcnn_resnet50_fpn(weights=weights)\n",
        "# Chuyển mô hình sang thiết bị và đặt ở chế độ đánh giá\n",
        "maskrcnn_model = maskrcnn_model.to(DEVICE).eval()\n",
        "print(\">> Model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b67f4f2",
      "metadata": {
        "id": "5b67f4f2"
      },
      "source": [
        "# Extract features for video captioning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "985f4559",
      "metadata": {
        "id": "985f4559"
      },
      "source": [
        "## Inception-ResNet-V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f5b73191",
      "metadata": {
        "id": "f5b73191"
      },
      "outputs": [],
      "source": [
        "def extract_image_features(video_frames):\n",
        "    \"\"\"\n",
        "    Trích xuất đặc trưng hình ảnh từ danh sách khung hình (BGR numpy arrays).\n",
        "    Trả về mảng NumPy có shape (num_frames, feature_dim).\n",
        "    \"\"\"\n",
        "    # Danh sách để lưu đặc trưng\n",
        "    all_image_features = []\n",
        "\n",
        "    # Hàm tiền xử lý khung hình\n",
        "    def preprocess_frame(frame_bgr):\n",
        "        \"\"\" Tiền xử lý một khung hình (BGR numpy array) cho Inception-ResNet-V2. \"\"\"\n",
        "        # Chuyển BGR sang RGB\n",
        "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        # Thay đổi kích thước về 299x299\n",
        "        frame_resized = cv2.resize(frame_rgb, (299, 299))\n",
        "        # Chuyển thành mảng numpy float32\n",
        "        img_array = np.asarray(frame_resized, dtype=np.float32)\n",
        "        # Mở rộng chiều để tạo batch (batch_size, height, width, channels)\n",
        "        img_array_batch = np.expand_dims(img_array, axis=0)\n",
        "        # Tiền xử lý theo chuẩn ImageNet (chuẩn hóa pixel)\n",
        "        preprocessed_img = preprocess_input(img_array_batch)\n",
        "        return preprocessed_img\n",
        "\n",
        "    print(\"Extracting features from frames...\")\n",
        "    for i, frame in enumerate(video_frames):\n",
        "        # In mỗi 10 frames hoặc với i == 0\n",
        "        if (i + 1) % 10 == 0 or i == 0:\n",
        "            print(f\">> Extract image feature for frame {i+1}/{len(video_frames)}\")\n",
        "        # Tiền xử lý khung hình\n",
        "        preprocessed_frame = preprocess_frame(frame)\n",
        "        # Trích xuất đặc trưng: features.shape sẽ là (1, 1536) do dùng pooling='avg'\n",
        "        features = inception_resnet_model.predict(\n",
        "            preprocessed_frame, verbose=0)  # verbose=0 để giảm log\n",
        "        # Loại bỏ chiều batch và thêm vào danh sách\n",
        "        all_image_features.append(features[0])  # features[0].shape là (1536,)\n",
        "\n",
        "    # Chuyển đổi danh sách đặc trưng thành mảng NumPy có shape (num_frames, 1536)\n",
        "    image_feats = np.array(all_image_features)\n",
        "    print(\"Feature extraction complete.\")\n",
        "\n",
        "    return image_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad625c4",
      "metadata": {
        "id": "5ad625c4"
      },
      "source": [
        "## Mask RCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "67ec825a",
      "metadata": {
        "id": "67ec825a"
      },
      "outputs": [],
      "source": [
        "# --- Cấu hình ---\n",
        "TOP_K_PER_FRAME = 20  # Số lượng detection tốt nhất được giữ lại *mỗi khung hình*\n",
        "COORD_MODE = \"cxcywh\"  # Chế độ mã hóa tọa độ bounding box\n",
        "CONFIDENCE_THRESHOLD = 0.7  # Ngưỡng confidence cho detection\n",
        "# Số lượng đặc trưng cuối cùng mong muốn (theo yêu cầu trước)\n",
        "TARGET_TOTAL_FEATURES = 50\n",
        "# --- Kết thúc Cấu hình ---\n",
        "\n",
        "# 4. Hàm tiền xử lý: Chuyển PIL Image -> Tensor (C,H,W), giá trị [0,1]\n",
        "preproc = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "def compute_box_coords(boxes, image_size, mode=\"cxcywh_log\"):\n",
        "    \"\"\"\n",
        "    Tính toán và chuẩn hóa tọa độ bounding box.\n",
        "    \"\"\"\n",
        "    H, W = image_size\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    w = (x2 - x1).clamp(min=1.0)\n",
        "    h = (y2 - y1).clamp(min=1.0)\n",
        "    cx = x1 + 0.5 * w\n",
        "    cy = y1 + 0.5 * h\n",
        "\n",
        "    if mode == \"xyxy_norm\":\n",
        "        coords = torch.stack([x1 / W, y1 / H, x2 / W, y2 / H], dim=1)\n",
        "    elif mode == \"cxcywh\":\n",
        "        coords = torch.stack([cx / W, cy / H, w / W, h / H], dim=1)\n",
        "    else:  # cxcywh_log (mặc định)\n",
        "        coords = torch.stack(\n",
        "            [cx / W, cy / H, torch.log(w / W), torch.log(h / H)], dim=1)\n",
        "    return coords\n",
        "\n",
        "\n",
        "@torch.no_grad()  # Tắt gradient để tăng tốc độ và tiết kiệm bộ nhớ trong suy luận\n",
        "def extract_instance_feats_with_coords(model, pil_img, topk=50, coord_mode=\"cxcywh\", conf_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Trích xuất đặc trưng 1028-D từ các đối tượng trong hình ảnh sử dụng Mask R-CNN.\n",
        "    Args:\n",
        "        model: Mô hình Mask R-CNN đã được tải.\n",
        "        pil_img: PIL Image.\n",
        "        topk: Số lượng detection tốt nhất được giữ lại.\n",
        "        coord_mode: Chế độ mã hóa tọa độ ('cxcywh', 'cxcywh_log', 'xyxy_norm').\n",
        "        conf_threshold: Ngưỡng confidence tối thiểu.\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor đặc trưng có shape (N, 1028).\n",
        "                      N là số lượng đối tượng được phát hiện (<= topk).\n",
        "    \"\"\"\n",
        "    img = pil_img.convert(\"RGB\")\n",
        "\n",
        "    # 1) Chạy mô hình để có detections (boxes, scores, labels, masks...)\n",
        "    # model.forward nhận danh sách tensor\n",
        "    img_tensor = preproc(img).to(DEVICE)  # Chuyển PIL -> Tensor và lên GPU\n",
        "    # Trả về danh sách kết quả cho từng ảnh trong batch\n",
        "    outputs = model([img_tensor])\n",
        "    out = outputs[0]  # Lấy kết quả cho ảnh đầu tiên (batch size = 1)\n",
        "\n",
        "    # Trích xuất boxes, scores, labels\n",
        "    boxes = out.get(\"boxes\", torch.empty(\n",
        "        (0, 4), device=DEVICE))  # Boxes trên device\n",
        "    scores = out.get(\"scores\", torch.empty((0,), device=DEVICE))\n",
        "    labels = out.get(\"labels\", torch.empty(\n",
        "        (0,), device=DEVICE))  # Có thể dùng nếu cần\n",
        "\n",
        "    # 2) Lọc theo ngưỡng confidence\n",
        "    if scores.numel() > 0 and conf_threshold > 0.0:\n",
        "        keep_conf = scores >= conf_threshold\n",
        "        boxes = boxes[keep_conf]\n",
        "        scores = scores[keep_conf]\n",
        "        labels = labels[keep_conf]  # Nếu dùng labels\n",
        "\n",
        "    # 3) Xử lý trường hợp không có detection nào sau khi lọc\n",
        "    if boxes.numel() == 0:\n",
        "        print(\"Warning: No objects detected (or survived filtering). Returning empty features.\")\n",
        "        # Trả về tensor rỗng với shape đúng\n",
        "        return torch.empty((0, 1028), dtype=torch.float32, device='cpu')\n",
        "\n",
        "    # 4) Chọn top-k detections dựa trên score\n",
        "    if scores is not None and scores.numel() > 0:\n",
        "        # torch.topk thường hiệu quả hơn argsort + slice nếu k << total\n",
        "        k = min(topk, boxes.shape[0])\n",
        "        top_scores, order = torch.topk(scores, k, largest=True, sorted=True)\n",
        "        keep = order\n",
        "        boxes = boxes[keep]\n",
        "        scores = scores[keep]\n",
        "        labels = labels[keep]  # Nếu dùng labels\n",
        "    else:\n",
        "        # Nếu không có scores hoặc scores rỗng (hiếm khi xảy ra)\n",
        "        boxes = boxes[:topk]\n",
        "\n",
        "    # --- Bắt đầu trích xuất đặc trưng ---\n",
        "    # 5) Tiền xử lý lại ảnh để đưa vào model.transform\n",
        "    # (model.transform xử lý normalization, resizing nếu cần)\n",
        "    # img_tensor đã được tạo ở trên\n",
        "    # images, _ = model.transform([img_tensor]) # Có thể dùng nếu cần transform lại\n",
        "    # Tuy nhiên, vì img_tensor đã được chuẩn hóa đúng cách bởi preproc và model.transform\n",
        "    # sẽ không làm gì thêm nếu kích thước phù hợp và không có chuyển đổi khác,\n",
        "    # ta có thể bỏ qua bước này và dùng trực tiếp img_tensor.\n",
        "    # Nhưng để đúng logic và chắc chắn, ta vẫn gọi transform.\n",
        "    # img_tensor_for_transform = preproc(img).unsqueeze(0) # Thêm batch dim\n",
        "    images, _ = model.transform([img_tensor])  # Trả về ImageList\n",
        "    images_t = images.tensors.to(DEVICE)    # (1, 3, H', W')\n",
        "    # [(H', W')] - kích thước sau transform\n",
        "    image_sizes = images.image_sizes\n",
        "\n",
        "    # 6) Backbone -> features dict\n",
        "    features = model.backbone(images_t)  # OrderedDict of feature maps\n",
        "\n",
        "    # 7) ROI pooling (boxes phải ở không gian tọa độ ảnh gốc, đã được transform xử lý)\n",
        "    # boxes vẫn đang ở device\n",
        "    pooled = model.roi_heads.box_roi_pool(\n",
        "        features, [boxes], image_sizes)  # (N, 256, 7, 7)\n",
        "\n",
        "    # 8) box head -> (N, 1024)\n",
        "    # Đây là bước quan trọng để lấy feature vector 1024-D\n",
        "    box_repr = model.roi_heads.box_head(pooled)  # (N, 1024)\n",
        "\n",
        "    # 9) Tính toán và chuẩn hóa tọa độ\n",
        "    H_img, W_img = image_sizes[0]  # ints - kích thước ảnh sau khi transform\n",
        "    coords = compute_box_coords(boxes, (H_img, W_img), mode=coord_mode)\n",
        "    coords = coords.to(box_repr.dtype).to(\n",
        "        box_repr.device)  # Đảm bảo dtype và device khớp\n",
        "\n",
        "    # 10) Kết hợp đặc trưng và tọa độ -> (N, 1028)\n",
        "    feat1028 = torch.cat([box_repr, coords], dim=1)  # (N, 1028)\n",
        "\n",
        "    return feat1028.cpu()  # Chuyển về CPU để dễ xử lý sau này\n",
        "\n",
        "\n",
        "def aggregate_features(feature_list, target_num_features):\n",
        "    \"\"\"\n",
        "    Tổng hợp danh sách các tensor đặc trưng thành một tensor cố định.\n",
        "    Args:\n",
        "        feature_list: List of torch.Tensor, each with shape (N_i, 1028).\n",
        "        target_num_features: int, số lượng đặc trưng mong muốn.\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor với shape (target_num_features, 1028).\n",
        "    \"\"\"\n",
        "    if not feature_list:\n",
        "        print(\"Warning: feature_list is empty. Returning zero tensor.\")\n",
        "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
        "\n",
        "    # Gộp tất cả các đặc trưng từ các khung hình\n",
        "    # Bỏ qua các tensor rỗng (0, 1028) nếu có\n",
        "    non_empty_features = [f for f in feature_list if f.shape[0] > 0]\n",
        "\n",
        "    if not non_empty_features:\n",
        "        print(\"Warning: No features detected in any frame. Returning zero tensor.\")\n",
        "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
        "\n",
        "    # Shape: (Total_Detections, 1028)\n",
        "    all_features = torch.cat(non_empty_features, dim=0)\n",
        "    total_detections = all_features.shape[0]\n",
        "    print(\n",
        "        f\"Total object detections across all sampled frames: {total_detections}\")\n",
        "\n",
        "    if total_detections == 0:\n",
        "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
        "\n",
        "    if total_detections >= target_num_features:\n",
        "        # Lấy mẫu đều để giảm xuống target_num_features\n",
        "        indices = np.linspace(0, total_detections - 1,\n",
        "                              target_num_features, dtype=int)\n",
        "        selected_features = all_features[indices]\n",
        "    else:\n",
        "        # Nếu không đủ, pad bằng cách lặp lại đặc trưng cuối cùng\n",
        "        print(\n",
        "            f\"Warning: Only {total_detections} features found. Padding to {target_num_features}.\")\n",
        "        num_to_pad = target_num_features - total_detections\n",
        "        if num_to_pad > 0:\n",
        "            # [num_to_pad, 1028]\n",
        "            padding = all_features[-1:].repeat(num_to_pad, 1)\n",
        "            # [target_num_features, 1028]\n",
        "            selected_features = torch.cat([all_features, padding], dim=0)\n",
        "        else:\n",
        "            selected_features = all_features\n",
        "\n",
        "    # Đảm bảo shape cuối cùng chính xác\n",
        "    assert selected_features.shape == (\n",
        "        target_num_features, 1028), f\"Aggregation failed: {selected_features.shape}\"\n",
        "    return selected_features  # Shape: (target_num_features, 1028)\n",
        "\n",
        "\n",
        "def extract_object_features_from_video(video_frames_bgr):\n",
        "    \"\"\"\n",
        "    Trích xuất đặc trưng đối tượng từ danh sách khung hình video (BGR numpy arrays).\n",
        "    Args:\n",
        "        video_frames_bgr: List of BGR numpy arrays.\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor đặc trưng có shape (1, TARGET_TOTAL_FEATURES, 1028).\n",
        "    \"\"\"\n",
        "\n",
        "    # Tạo placeholder cho relationship features, kích thước (50, 300)\n",
        "    relationship_feats = torch.zeros((TARGET_TOTAL_FEATURES, 300), dtype=torch.float32)\n",
        "\n",
        "    # Danh sách lưu trữ đặc trưng từ từng khung hình\n",
        "    all_frame_features = []\n",
        "\n",
        "    # Lặp qua từng khung hình BGR\n",
        "    print(\"Extracting features from each frame...\")\n",
        "    for i, frame_bgr in enumerate(video_frames_bgr):\n",
        "        # In mỗi 10 frames hoặc với i == 0\n",
        "        if (i + 1) % 10 == 0 or i == 0:\n",
        "            print(f\"  Processing frame {i+1}/{len(video_frames_bgr)}\")\n",
        "        # Chuyển đổi BGR (OpenCV) -> RGB -> PIL Image\n",
        "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        pil_img = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # Trích xuất đặc trưng từ khung hình\n",
        "        try:\n",
        "            frame_features = extract_instance_feats_with_coords(\n",
        "                maskrcnn_model, pil_img,\n",
        "                topk=TOP_K_PER_FRAME,\n",
        "                coord_mode=COORD_MODE,\n",
        "                conf_threshold=CONFIDENCE_THRESHOLD\n",
        "            )\n",
        "            # Thêm tensor (N_i, 1028) hoặc (0, 1028)\n",
        "            all_frame_features.append(frame_features)\n",
        "        except Exception as e:\n",
        "            print(f\"    Error extracting features from frame {i+1}: {e}\")\n",
        "            # Thêm tensor rỗng nếu lỗi\n",
        "            all_frame_features.append(\n",
        "                torch.empty((0, 1028), dtype=torch.float32))\n",
        "\n",
        "    # Tổng hợp đặc trưng để có số lượng cố định\n",
        "    print(\"Aggregating features to target shape...\")\n",
        "    object_feats = aggregate_features(\n",
        "        all_frame_features, TARGET_TOTAL_FEATURES)  # Tensor (50, 1028)\n",
        "\n",
        "    return object_feats, relationship_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b844036",
      "metadata": {
        "id": "2b844036"
      },
      "source": [
        "## I3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "777d29c9",
      "metadata": {
        "id": "777d29c9"
      },
      "outputs": [],
      "source": [
        "def extract_motion_features(video_path):\n",
        "    # Select the feature type\n",
        "    feature_type = 'i3d'\n",
        "\n",
        "    # Load and patch the config\n",
        "    args = OmegaConf.load(build_cfg_path(feature_type))\n",
        "    args.video_paths = [video_path]\n",
        "    # args.show_pred = True\n",
        "    args.stack_size = 64\n",
        "    args.step_size = 64\n",
        "    args.extraction_fps = 50\n",
        "    args.flow_type = 'raft'\n",
        "    # args.streams = 'flow'\n",
        "\n",
        "    # Load the model\n",
        "    extractor = ExtractI3D(args)\n",
        "\n",
        "    # Extract features\n",
        "    for video_path in args.video_paths:\n",
        "        print(f'Extracting for {video_path}')\n",
        "        feature_dict = extractor.extract(video_path)\n",
        "\n",
        "    # Kết hợp đặc trưng RGB và Flow từ 2 stream\n",
        "    motion_feats = feature_dict['rgb'] + feature_dict['flow']\n",
        "    return motion_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074f4499",
      "metadata": {
        "id": "074f4499"
      },
      "source": [
        "## Full pipeline for feature extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1a7530a7",
      "metadata": {
        "id": "1a7530a7"
      },
      "outputs": [],
      "source": [
        "def sample_frames(video_path, target_fps):\n",
        "    \"\"\" Hàm lấy mẫu khung hình từ video ở fps mục tiêu.\n",
        "    Ảnh trả về là danh sách các mảng numpy (BGR).\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    interval = max(1, int(round(orig_fps / target_fps)))\n",
        "    frames = []\n",
        "    idx = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if idx % interval == 0:\n",
        "            # Giữ nguyên ở dạng BGR hoặc chuyển sang RGB nếu mô hình yêu cầu (InceptionResNetV2 dùng RGB)\n",
        "            # frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) # Nếu cần RGB\n",
        "            frames.append(frame)  # Giữ BGR, sẽ xử lý sau\n",
        "        idx += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def resample_fixed(feats, N):\n",
        "    \"\"\" Hàm lấy mẫu lại chuỗi đặc trưng để có đúng N đặc trưng bằng cách lấy mẫu đều.\n",
        "    \"\"\"\n",
        "    T, D = feats.shape\n",
        "    idxs = np.linspace(0, T-1, N).astype(int)\n",
        "    return feats[idxs]\n",
        "\n",
        "\n",
        "def process_video(video_path, dataset='MSVD'):\n",
        "    \"\"\" Full pipeline để xử lý video và trích xuất đặc trưng.\n",
        "    \"\"\"\n",
        "    N = 50 if dataset == 'MSVD' else 60\n",
        "    target_fps = 5 if dataset == 'MSVD' else 3\n",
        "\n",
        "    # --- Tải video và lấy mẫu frames ---\n",
        "    print(\"Sampling video frames...\")\n",
        "    # Danh sách các mảng numpy (BGR)\n",
        "    video_frames = sample_frames(video_path, target_fps)\n",
        "    print(f\"Sampled {len(video_frames)} frames at ~{target_fps} fps.\")\n",
        "\n",
        "    # Nếu có nhiều hơn 50 frames, lấy 50 frames cách đều nhau để giảm bớt\n",
        "    if len(video_frames) > 50:\n",
        "        indices = np.linspace(0, len(video_frames) - 1, 50, dtype=int)\n",
        "        video_frames = [video_frames[i] for i in indices]\n",
        "        print(f\"Reduced to {len(video_frames)} frames for processing.\")\n",
        "\n",
        "    # Trích xuất đặc trưng với mô hình Inception-ResNet-V2\n",
        "    image_feats = extract_image_features(video_frames)\n",
        "    # Trích xuất đặc trưng với mô hình Mask R-CNN\n",
        "    object_feats, rel_feats = extract_object_features_from_video(video_frames)\n",
        "    # Trích xuất đặc trưng với mô hình I3D\n",
        "    motion_feats = extract_motion_features(video_path)\n",
        "\n",
        "    # Resample để có đúng N đặc trưng\n",
        "    image_feats_res = resample_fixed(image_feats, N)  # (N, 1536)\n",
        "    object_feats_res = resample_fixed(object_feats.numpy(), N)  # (N, 1028)\n",
        "    rel_feats_res = resample_fixed(rel_feats.numpy(), N)  # (N, 300)\n",
        "    motion_feats_res = resample_fixed(motion_feats, N)  # (N, 1024)\n",
        "\n",
        "    # Đảm bảo tất cả đặc trưng đều có kiểu dữ liệu float32\n",
        "    image_feats_res = image_feats_res.astype(np.float32)\n",
        "    object_feats_res = object_feats_res.astype(np.float32)\n",
        "    rel_feats_res = rel_feats_res.astype(np.float32)\n",
        "    motion_feats_res = motion_feats_res.astype(np.float32)\n",
        "\n",
        "    # Chuyển sang tensor và thêm batch dim\n",
        "    image_feats_tensor = torch.from_numpy(image_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 1536)\n",
        "    object_feats_tensor = torch.from_numpy(object_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 1028)\n",
        "    rel_feats_tensor = torch.from_numpy(rel_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 300)\n",
        "    motion_feats_tensor = torch.from_numpy(motion_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 1024)\n",
        "\n",
        "    # Trả về kết quả\n",
        "    return {\n",
        "        'image_feats': image_feats_tensor,\n",
        "        'motion_feats': motion_feats_tensor,\n",
        "        'object_feats': object_feats_tensor,\n",
        "        'rel_feats': rel_feats_tensor\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8331fd7c",
      "metadata": {
        "id": "8331fd7c"
      },
      "source": [
        "# Load checkpoint and config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4281485a",
      "metadata": {
        "id": "4281485a"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"checkpoints/best.ckpt\", map_location=\"cpu\")\n",
        "config = dict_to_cls(checkpoint['config'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "86d47827",
      "metadata": {
        "id": "86d47827"
      },
      "outputs": [],
      "source": [
        "corpus = MSVD(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2d87b1",
      "metadata": {
        "id": "ec2d87b1"
      },
      "source": [
        "# Build Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "65f8ff2e",
      "metadata": {
        "id": "65f8ff2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcd7d26-8191-45cc-cbb4-825dbf9f70e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "vocab = corpus.vocab\n",
        "\"\"\" Build Models \"\"\"\n",
        "try:\n",
        "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
        "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
        "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big,\n",
        "                           select_num=config.transformer.select_num)\n",
        "except:\n",
        "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
        "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
        "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big)\n",
        "model.load_state_dict(checkpoint['abd_transformer'])\n",
        "model.device = DEVICE\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(DEVICE)\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44af2738",
      "metadata": {
        "id": "44af2738"
      },
      "source": [
        "# Inference with beam search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fe3d06f6",
      "metadata": {
        "id": "fe3d06f6"
      },
      "outputs": [],
      "source": [
        "def generate_video_caption(video_path):\n",
        "    # Rút trích đặc trưng từ video\n",
        "    feats_dict = process_video(video_path)\n",
        "    feats = (\n",
        "        feats_dict['image_feats'],\n",
        "        feats_dict['motion_feats'],\n",
        "        feats_dict['object_feats'],\n",
        "        feats_dict['rel_feats']\n",
        "    )\n",
        "    # Tạo caption cho video\n",
        "    model.eval()\n",
        "    beam_size = config.beam_size\n",
        "    max_len = config.loader.max_caption_len\n",
        "    with torch.no_grad():\n",
        "        r2l_captions, l2r_captions = model.beam_search_decode(feats, beam_size, max_len)\n",
        "        # r2l_captions = [idxs_to_sentence(caption, vocab.idx2word, BOS_idx) for caption in r2l_captions]\n",
        "        l2r_captions = [\" \".join(caption[0].value) for caption in l2r_captions]\n",
        "        r2l_captions = [\" \".join(caption[0].value) for caption in r2l_captions]\n",
        "\n",
        "    print(f\"Left to Right Captions: {l2r_captions}\")\n",
        "    return l2r_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cf74303e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf74303e",
        "outputId": "3a4f0c82-419b-4caf-9008-f8e679e28446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating caption for bento.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 25 frames at ~5 fps.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/25\n",
            ">> Extract image feature for frame 10/25\n",
            ">> Extract image feature for frame 20/25\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/25\n",
            "  Processing frame 10/25\n",
            "  Processing frame 20/25\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 106\n",
            "Extracting for videos/bento.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a person is putting a piece of food into a']\n",
            ">> Captions for bento.mp4: ['a person is putting a piece of food into a']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for lifting.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 89 frames at ~5 fps.\n",
            "Reduced to 50 frames for processing.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/50\n",
            ">> Extract image feature for frame 10/50\n",
            ">> Extract image feature for frame 20/50\n",
            ">> Extract image feature for frame 30/50\n",
            ">> Extract image feature for frame 40/50\n",
            ">> Extract image feature for frame 50/50\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/50\n",
            "  Processing frame 10/50\n",
            "  Processing frame 20/50\n",
            "  Processing frame 30/50\n",
            "  Processing frame 40/50\n",
            "  Processing frame 50/50\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 58\n",
            "Extracting for videos/lifting.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a man is lifting a weight']\n",
            ">> Captions for lifting.mp4: ['a man is lifting a weight']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for slicing_carrot.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 30 frames at ~5 fps.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/30\n",
            ">> Extract image feature for frame 10/30\n",
            ">> Extract image feature for frame 20/30\n",
            ">> Extract image feature for frame 30/30\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/30\n",
            "  Processing frame 10/30\n",
            "  Processing frame 20/30\n",
            "  Processing frame 30/30\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 111\n",
            "Extracting for videos/slicing_carrot.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a woman is cutting a tomato']\n",
            ">> Captions for slicing_carrot.mp4: ['a woman is cutting a tomato']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for walking_the_dog.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 70 frames at ~5 fps.\n",
            "Reduced to 50 frames for processing.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/50\n",
            ">> Extract image feature for frame 10/50\n",
            ">> Extract image feature for frame 20/50\n",
            ">> Extract image feature for frame 30/50\n",
            ">> Extract image feature for frame 40/50\n",
            ">> Extract image feature for frame 50/50\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/50\n",
            "  Processing frame 10/50\n",
            "  Processing frame 20/50\n",
            "  Processing frame 30/50\n",
            "  Processing frame 40/50\n",
            "  Processing frame 50/50\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 214\n",
            "Extracting for videos/walking_the_dog.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a man is running']\n",
            ">> Captions for walking_the_dog.mp4: ['a man is running']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for slicing_cucumber.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 35 frames at ~5 fps.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/35\n",
            ">> Extract image feature for frame 10/35\n",
            ">> Extract image feature for frame 20/35\n",
            ">> Extract image feature for frame 30/35\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/35\n",
            "  Processing frame 10/35\n",
            "  Processing frame 20/35\n",
            "  Processing frame 30/35\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 74\n",
            "Extracting for videos/slicing_cucumber.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a woman is slicing a cake']\n",
            ">> Captions for slicing_cucumber.mp4: ['a woman is slicing a cake']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for pour_water_to_rice.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 55 frames at ~5 fps.\n",
            "Reduced to 50 frames for processing.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/50\n",
            ">> Extract image feature for frame 10/50\n",
            ">> Extract image feature for frame 20/50\n",
            ">> Extract image feature for frame 30/50\n",
            ">> Extract image feature for frame 40/50\n",
            ">> Extract image feature for frame 50/50\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/50\n",
            "  Processing frame 10/50\n",
            "  Processing frame 20/50\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  Processing frame 30/50\n",
            "  Processing frame 40/50\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  Processing frame 50/50\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 66\n",
            "Extracting for videos/pour_water_to_rice.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a man is cooking']\n",
            ">> Captions for pour_water_to_rice.mp4: ['a man is cooking']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for fry_meet.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 65 frames at ~5 fps.\n",
            "Reduced to 50 frames for processing.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/50\n",
            ">> Extract image feature for frame 10/50\n",
            ">> Extract image feature for frame 20/50\n",
            ">> Extract image feature for frame 30/50\n",
            ">> Extract image feature for frame 40/50\n",
            ">> Extract image feature for frame 50/50\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/50\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  Processing frame 10/50\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  Processing frame 20/50\n",
            "  Processing frame 30/50\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  Processing frame 40/50\n",
            "  Processing frame 50/50\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 84\n",
            "Extracting for videos/fry_meet.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a person is cooking']\n",
            ">> Captions for fry_meet.mp4: ['a person is cooking']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for fry_egg.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 40 frames at ~5 fps.\n",
            "Extracting features from frames...\n",
            ">> Extract image feature for frame 1/40\n",
            ">> Extract image feature for frame 10/40\n",
            ">> Extract image feature for frame 20/40\n",
            ">> Extract image feature for frame 30/40\n",
            ">> Extract image feature for frame 40/40\n",
            "Feature extraction complete.\n",
            "Extracting features from each frame...\n",
            "  Processing frame 1/40\n",
            "  Processing frame 10/40\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  Processing frame 20/40\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  Processing frame 30/40\n",
            "  Processing frame 40/40\n",
            "Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 74\n",
            "Extracting for videos/fry_egg.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left to Right Captions: ['a woman is cooking']\n",
            ">> Captions for fry_egg.mp4: ['a woman is cooking']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Duyệt qua từng video trong folder 'videos' và tạo caption\n",
        "import os\n",
        "for filename in os.listdir('videos'):\n",
        "    video_path = os.path.join('videos', filename)\n",
        "    print(f\"Generating caption for {filename}...\")\n",
        "    captions = generate_video_caption(video_path)\n",
        "    print(f\">> Captions for {filename}: {captions}\\n\")\n",
        "    print(\"--------------------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jk807SC6PgNY"
      },
      "id": "jk807SC6PgNY",
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}