{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca90194",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e8b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import TrainConfig as C\n",
    "from models.abd_transformer import ABDTransformer\n",
    "import torch\n",
    "import h5py\n",
    "from utils import dict_to_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331fd7c",
   "metadata": {},
   "source": [
    "# Load checkpoint and config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4281485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoints/best.ckpt\", map_location=\"cpu\")\n",
    "config = dict_to_cls(checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af687194",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d47827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader.MSVD import MSVD\n",
    "corpus = MSVD(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d87b1",
   "metadata": {},
   "source": [
    "# Build Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65f8ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = corpus.vocab\n",
    "\"\"\" Build Models \"\"\"\n",
    "try:\n",
    "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
    "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
    "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big,\n",
    "                           select_num=config.transformer.select_num)\n",
    "except:\n",
    "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
    "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
    "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big)\n",
    "model.load_state_dict(checkpoint['abd_transformer'])\n",
    "model.device = \"cpu\"\n",
    "\n",
    "# Move model to cpu\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0eed1",
   "metadata": {},
   "source": [
    "# Load extracted features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e0f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in HDF5 file: ['lifting']\n",
      "Using key: lifting\n",
      "Motion features shape before unsqueeze: torch.Size([50, 1024])\n",
      "Motion features shape after unsqueeze: torch.Size([1, 50, 1024])\n",
      "Image features shape: torch.Size([1, 50, 1536])\n",
      "Motion features shape: torch.Size([1, 50, 1024])\n",
      "Object features shape: torch.Size([1, 50, 1028])\n",
      "Relation features shape: torch.Size([1, 50, 300])\n"
     ]
    }
   ],
   "source": [
    "# Load saved features\n",
    "image_feats = torch.load('features/image_feats.pt', map_location=\"cpu\")\n",
    "\n",
    "# Load motion features from HDF5 file with error handling\n",
    "try:\n",
    "    with h5py.File('features/motion_feats.hdf5', 'r') as f:\n",
    "        # Print available keys to see the structure\n",
    "        print(\"Available keys in HDF5 file:\", list(f.keys()))\n",
    "        \n",
    "        # Try common dataset key names\n",
    "        if 'features' in f:\n",
    "            motion_feats = torch.tensor(f['features'][:], dtype=torch.float32)\n",
    "        elif 'motion_features' in f:\n",
    "            motion_feats = torch.tensor(f['motion_features'][:], dtype=torch.float32)\n",
    "        elif 'data' in f:\n",
    "            motion_feats = torch.tensor(f['data'][:], dtype=torch.float32)\n",
    "        else:\n",
    "            # Use the first available key\n",
    "            first_key = list(f.keys())[0]\n",
    "            print(f\"Using key: {first_key}\")\n",
    "            motion_feats = torch.tensor(f[first_key][:], dtype=torch.float32)\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(\"HDF5 file not found, falling back to .pt file\")\n",
    "    motion_feats = torch.load('features/motion_feats.pt', map_location=\"cpu\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading HDF5 file: {e}\")\n",
    "    print(\"Falling back to .pt file\")\n",
    "    motion_feats = torch.load('features/motion_feats.pt', map_location=\"cpu\")\n",
    "\n",
    "# Add batch dimension if needed (for shape [50, 1024] -> [1, 50, 1024])\n",
    "print(f\"Motion features shape before unsqueeze: {motion_feats.shape}\")\n",
    "if len(motion_feats.shape) == 2:\n",
    "    motion_feats = motion_feats.unsqueeze(0)\n",
    "    print(f\"Motion features shape after unsqueeze: {motion_feats.shape}\")\n",
    "\n",
    "obect_feats = torch.load('features/object_feats.pt', map_location=\"cpu\")\n",
    "rel_feats = torch.load('features/rel_feats.pt', map_location=\"cpu\")\n",
    "\n",
    "print(\"Image features shape:\", image_feats.shape)\n",
    "print(\"Motion features shape:\", motion_feats.shape)\n",
    "print(\"Object features shape:\", obect_feats.shape)\n",
    "print(\"Relation features shape:\", rel_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af2738",
   "metadata": {},
   "source": [
    "# Inference with beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3d06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left to Right Captions: ['the person is doing the something']\n",
      "CPU times: total: 8.28 s\n",
      "Wall time: 6.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "beam_size = config.beam_size\n",
    "max_len = config.loader.max_caption_len\n",
    "feature_mode = config.feat.feature_mode\n",
    "feats = (image_feats, motion_feats, obect_feats, rel_feats)\n",
    "with torch.no_grad():\n",
    "    r2l_captions, l2r_captions = model.beam_search_decode(feats, beam_size, max_len)\n",
    "    # r2l_captions = [idxs_to_sentence(caption, vocab.idx2word, BOS_idx) for caption in r2l_captions]\n",
    "    l2r_captions = [\" \".join(caption[0].value) for caption in l2r_captions]\n",
    "    r2l_captions = [\" \".join(caption[0].value) for caption in r2l_captions]\n",
    "    \n",
    "    print(f\"Left to Right Captions: {l2r_captions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
