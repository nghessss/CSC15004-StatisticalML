{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ou0YZkMt0K6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou0YZkMt0K6b",
        "outputId": "2468afe0-f817-4bce-8cf4-ddc6ee607372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5dl2kh820epq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dl2kh820epq",
        "outputId": "7376c034-4be0-4160-dfcc-f1ec4b72df50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/btkg\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/btkg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c66MfBmX0lM6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66MfBmX0lM6",
        "outputId": "cf247225-dd2a-4fe4-fd88-1b1db854be4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints   models\t     test_inference.ipynb   video_feature_utils\n",
            "config.py     __pycache__    tmp\t\t    videos\n",
            "data\t      pycocoevalcap  train.py\n",
            "inference.py  run.py\t     utils.py\n",
            "loader\t      splits\t     video_feature_configs\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ca90194",
      "metadata": {
        "id": "1ca90194"
      },
      "source": [
        "# Import necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "70e8b632",
      "metadata": {
        "id": "70e8b632"
      },
      "outputs": [],
      "source": [
        "# For video captioning\n",
        "from loader.MSVD import MSVD\n",
        "from config import TrainConfig as C\n",
        "from models.abd_transformer import ABDTransformer\n",
        "import torch\n",
        "from utils import dict_to_cls\n",
        "# Inception-ResNet-V2 for image feature extraction\n",
        "import os\n",
        "import cv2 # Thêm cv2 để đọc video\n",
        "import numpy as np # Thêm numpy để xử lý mảng\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "# I3D for motion feature extraction\n",
        "from models.i3d.extract_i3d import ExtractI3D\n",
        "from video_feature_utils.utils import build_cfg_path\n",
        "from omegaconf import OmegaConf\n",
        "# Mask R-CNN for object feature extraction\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987dc0b9",
      "metadata": {
        "id": "987dc0b9"
      },
      "source": [
        "# Thiết lập chung\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d89d9959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d89d9959",
        "outputId": "5c84d336-52c1-4fcd-d383-f9dde84904bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda'), 'three')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Thiết lập chung\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "FEATURE_MODE = \"three\"\n",
        "DEVICE, FEATURE_MODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "887d81ea",
      "metadata": {
        "id": "887d81ea"
      },
      "source": [
        "# Load pretrained model for feature extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b8ef5636",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "9ef68951ebd0447ba737d511cc312b38",
            "23fb3ea7405d4815a8a43cbe9256d4af",
            "8ed456d00de5480fbdc4acb4eb6492ab",
            "3a3de7daffb042a48dfdfae819becfc6",
            "190aa24befec423abc7aadb11b71abd5",
            "524dfc225c8a40b3aeb4d399db95cd28",
            "f1abe7f4ea8149848db27d6f89056f3e",
            "2418933a2dd341b6ac8137be6c1f93da",
            "8b8159df80d544b8a5e763b2d356d7a5",
            "85b54471a4e0403a9860fe06c77f2fdc",
            "c62cafce06b64c12831feecfe210310f"
          ]
        },
        "id": "b8ef5636",
        "outputId": "62123e73-573a-4fd7-aab9-fc71d6cc7696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Inception-ResNet-V2 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/224M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ef68951ebd0447ba737d511cc312b38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Model loaded.\n",
            "Loading Mask R-CNN model...\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:00<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Model loaded.\n"
          ]
        }
      ],
      "source": [
        "# --- Tải mô hình Inception-ResNet-V2 ---\n",
        "print(\"Loading Inception-ResNet-V2 model...\")\n",
        "# Load Inception-ResNet-V2 (pretrained) từ timm cho image features\n",
        "# num_classes=0 loại bỏ lớp phân loại cuối cùng, trả về feature vector\n",
        "inception_resnet_model = timm.create_model(\"inception_resnet_v2\", pretrained=True, features_only=False)\n",
        "inception_resnet_model.to(DEVICE).eval()\n",
        "print(\">> Model loaded.\")\n",
        "\n",
        "# --- Tải mô hình Mask R-CNN ---\n",
        "print(\"Loading Mask R-CNN model...\")\n",
        "# Tải mô hình Mask R-CNN được huấn luyện sẵn trên COCO\n",
        "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "maskrcnn_model = maskrcnn_resnet50_fpn(weights=weights)\n",
        "# Chuyển mô hình sang thiết bị và đặt ở chế độ đánh giá\n",
        "maskrcnn_model = maskrcnn_model.to(DEVICE).eval()\n",
        "print(\">> Model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b67f4f2",
      "metadata": {
        "id": "5b67f4f2"
      },
      "source": [
        "# Extract features for video captioning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "985f4559",
      "metadata": {
        "id": "985f4559"
      },
      "source": [
        "## Inception-ResNet-V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f5b73191",
      "metadata": {
        "id": "f5b73191"
      },
      "outputs": [],
      "source": [
        "def extract_image_features(video_frames_bgr):\n",
        "    \"\"\"\n",
        "    Trích xuất đặc trưng hình ảnh từ danh sách khung hình (BGR numpy arrays).\n",
        "    Trả về mảng NumPy có shape (num_frames, feature_dim).\n",
        "    \"\"\"\n",
        "\n",
        "    # Hàm tiền xử lý cho timm/PyTorch\n",
        "    # Lấy thông tin chuẩn hóa từ config của mô hình\n",
        "    data_config = timm.data.resolve_model_data_config(inception_resnet_model)\n",
        "    # Tạo bộ transform chuẩn dựa trên config\n",
        "    # transforms_timm = timm.data.create_transform(**data_config, is_training=False)\n",
        "    transforms_timm = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),    # InceptionResNetV2 expects 299x299\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  # timm models sometimes expect this; if you used different model change accordingly\n",
        "    ])\n",
        "\n",
        "    # Danh sách để lưu đặc trưng\n",
        "    all_image_features = []\n",
        "\n",
        "    print(\"Extracting image features using timm Inception-ResNet-V2...\")\n",
        "    for i, frame_bgr in enumerate(video_frames_bgr):\n",
        "        # In mỗi 10 frames hoặc với i == 0\n",
        "        if (i + 1) % 10 == 0 or i == 0:\n",
        "            print(f\"  >> Extract image feature for frame {i+1}/{len(video_frames_bgr)}\")\n",
        "\n",
        "        # 1. Chuyển BGR (OpenCV) sang RGB\n",
        "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB) # numpy array (H, W, C) RGB\n",
        "\n",
        "        # 2. CHUYỂN ĐỔI SANG PIL IMAGE TRƯỚC KHI TIỀN XỬ LÝ\n",
        "        # Chuyển numpy array RGB -> PIL Image\n",
        "        pil_image = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # 3. Tiền xử lý bằng transform của timm\n",
        "        # Giờ truyền PIL Image vào transform\n",
        "        try:\n",
        "            # Áp dụng transform lên PIL Image\n",
        "            input_tensor = transforms_timm(pil_image) # Trả về torch.Tensor (C, H, W)\n",
        "            # Thêm chiều batch\n",
        "            input_batch = input_tensor.unsqueeze(0).to(DEVICE) # (1, C, H, W)\n",
        "        except Exception as e:\n",
        "            print(f\"Error preprocessing frame {i+1}: {e}\")\n",
        "            # Thêm vector zero nếu lỗi\n",
        "            all_image_features.append(np.zeros(1536, dtype=np.float32))\n",
        "            continue\n",
        "\n",
        "        # 4. Trích xuất đặc trưng (phần còn lại giữ nguyên)\n",
        "        with torch.no_grad():\n",
        "            # features_tensor = inception_resnet_model(input_batch) # (1, 1536)\n",
        "            features_tensor = inception_resnet_model.forward_features(input_batch) # (1, 1536)\n",
        "\n",
        "        if features_tensor.ndim == 4:\n",
        "            features_tensor = torch.flatten(torch.nn.functional.adaptive_avg_pool2d(features_tensor, (1,1)), 1)\n",
        "\n",
        "        # 5. Chuyển tensor PyTorch -> numpy array và loại bỏ chiều batch\n",
        "        features_np = features_tensor.cpu().numpy().squeeze(0) # (1536,)\n",
        "        all_image_features.append(features_np)\n",
        "\n",
        "    # 5. Chuyển đổi danh sách đặc trưng thành mảng NumPy có shape (num_frames, 1536)\n",
        "    if all_image_features:\n",
        "        image_feats = np.array(all_image_features) # Shape: (num_frames, 1536)\n",
        "    else:\n",
        "        print(\"[extract_image_features] No frames processed, returning empty array.\")\n",
        "        image_feats = np.empty((0, 1536), dtype=np.float32)\n",
        "    print(f\"[v] Image feature extraction complete. Shape: {image_feats.shape}\")\n",
        "\n",
        "    return image_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad625c4",
      "metadata": {
        "id": "5ad625c4"
      },
      "source": [
        "## Mask R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "67ec825a",
      "metadata": {
        "id": "67ec825a"
      },
      "outputs": [],
      "source": [
        "# --- Cấu hình ---\n",
        "TOP_K_PER_FRAME = 20  # Số lượng detection tốt nhất được giữ lại *mỗi khung hình*\n",
        "COORD_MODE = \"cxcywh\"  # Chế độ mã hóa tọa độ bounding box\n",
        "CONFIDENCE_THRESHOLD = 0.7  # Ngưỡng confidence cho detection\n",
        "# Số lượng đặc trưng cuối cùng mong muốn\n",
        "TARGET_TOTAL_FEATURES = 50\n",
        "# --- Kết thúc Cấu hình ---\n",
        "\n",
        "# 4. Hàm tiền xử lý: Chuyển PIL Image -> Tensor (C,H,W), giá trị [0,1]\n",
        "preproc = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "def compute_box_coords(boxes, image_size, mode=\"cxcywh_log\"):\n",
        "    \"\"\"\n",
        "    Tính toán và chuẩn hóa tọa độ bounding box.\n",
        "    \"\"\"\n",
        "    H, W = image_size\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    w = (x2 - x1).clamp(min=1.0)\n",
        "    h = (y2 - y1).clamp(min=1.0)\n",
        "    cx = x1 + 0.5 * w\n",
        "    cy = y1 + 0.5 * h\n",
        "\n",
        "    if mode == \"xyxy_norm\":\n",
        "        coords = torch.stack([x1 / W, y1 / H, x2 / W, y2 / H], dim=1)\n",
        "    elif mode == \"cxcywh\":\n",
        "        coords = torch.stack([cx / W, cy / H, w / W, h / H], dim=1)\n",
        "    else:  # cxcywh_log (mặc định)\n",
        "        coords = torch.stack(\n",
        "            [cx / W, cy / H, torch.log(w / W), torch.log(h / H)], dim=1)\n",
        "    return coords\n",
        "\n",
        "\n",
        "@torch.no_grad()  # Tắt gradient để tăng tốc độ và tiết kiệm bộ nhớ trong suy luận\n",
        "def extract_instance_feats_with_coords(model, pil_img, topk=50, coord_mode=\"cxcywh\", conf_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Trích xuất đặc trưng 1028-D từ các đối tượng trong hình ảnh sử dụng Mask R-CNN.\n",
        "    Args:\n",
        "        model: Mô hình Mask R-CNN đã được tải.\n",
        "        pil_img: PIL Image.\n",
        "        topk: Số lượng detection tốt nhất được giữ lại.\n",
        "        coord_mode: Chế độ mã hóa tọa độ ('cxcywh', 'cxcywh_log', 'xyxy_norm').\n",
        "        conf_threshold: Ngưỡng confidence tối thiểu.\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor đặc trưng có shape (N, 1028).\n",
        "                      N là số lượng đối tượng được phát hiện (<= topk).\n",
        "    \"\"\"\n",
        "    img = pil_img.convert(\"RGB\")\n",
        "\n",
        "    # 1) Chạy mô hình để có detections (boxes, scores, labels, masks...)\n",
        "    # model.forward nhận danh sách tensor\n",
        "    img_tensor = preproc(img).to(DEVICE)  # Chuyển PIL -> Tensor và lên GPU\n",
        "    # Trả về danh sách kết quả cho từng ảnh trong batch\n",
        "    outputs = model([img_tensor])\n",
        "    out = outputs[0]  # Lấy kết quả cho ảnh đầu tiên (batch size = 1)\n",
        "\n",
        "    # Trích xuất boxes, scores, labels\n",
        "    boxes = out.get(\"boxes\", torch.empty(\n",
        "        (0, 4), device=DEVICE))  # Boxes trên device\n",
        "    scores = out.get(\"scores\", torch.empty((0,), device=DEVICE))\n",
        "    labels = out.get(\"labels\", torch.empty(\n",
        "        (0,), device=DEVICE))  # Có thể dùng nếu cần\n",
        "\n",
        "    # 2) Lọc theo ngưỡng confidence\n",
        "    if scores.numel() > 0 and conf_threshold > 0.0:\n",
        "        keep_conf = scores >= conf_threshold\n",
        "        boxes = boxes[keep_conf]\n",
        "        scores = scores[keep_conf]\n",
        "        labels = labels[keep_conf]  # Nếu dùng labels\n",
        "\n",
        "    # 3) Xử lý trường hợp không có detection nào sau khi lọc\n",
        "    if boxes.numel() == 0:\n",
        "        print(\"Warning: No objects detected (or survived filtering). Returning empty features.\")\n",
        "        # Trả về tensor rỗng với shape đúng\n",
        "        return torch.empty((0, 1028), dtype=torch.float32, device='cpu')\n",
        "\n",
        "    # 4) Chọn top-k detections dựa trên score\n",
        "    if scores is not None and scores.numel() > 0:\n",
        "        # torch.topk thường hiệu quả hơn argsort + slice nếu k << total\n",
        "        k = min(topk, boxes.shape[0])\n",
        "        top_scores, order = torch.topk(scores, k, largest=True, sorted=True)\n",
        "        keep = order\n",
        "        boxes = boxes[keep]\n",
        "        scores = scores[keep]\n",
        "        labels = labels[keep]  # Nếu dùng labels\n",
        "    else:\n",
        "        # Nếu không có scores hoặc scores rỗng (hiếm khi xảy ra)\n",
        "        boxes = boxes[:topk]\n",
        "\n",
        "    # --- Bắt đầu trích xuất đặc trưng ---\n",
        "    # 5) Tiền xử lý lại ảnh để đưa vào model.transform\n",
        "    # (model.transform xử lý normalization, resizing nếu cần)\n",
        "    # img_tensor đã được tạo ở trên\n",
        "    # images, _ = model.transform([img_tensor]) # Có thể dùng nếu cần transform lại\n",
        "    # Tuy nhiên, vì img_tensor đã được chuẩn hóa đúng cách bởi preproc và model.transform\n",
        "    # sẽ không làm gì thêm nếu kích thước phù hợp và không có chuyển đổi khác,\n",
        "    # ta có thể bỏ qua bước này và dùng trực tiếp img_tensor.\n",
        "    # Nhưng để đúng logic và chắc chắn, ta vẫn gọi transform.\n",
        "    # img_tensor_for_transform = preproc(img).unsqueeze(0) # Thêm batch dim\n",
        "    images, _ = model.transform([img_tensor])  # Trả về ImageList\n",
        "    images_t = images.tensors.to(DEVICE)    # (1, 3, H', W')\n",
        "    # [(H', W')] - kích thước sau transform\n",
        "    image_sizes = images.image_sizes\n",
        "\n",
        "    # 6) Backbone -> features dict\n",
        "    features = model.backbone(images_t)  # OrderedDict of feature maps\n",
        "\n",
        "    # 7) ROI pooling (boxes phải ở không gian tọa độ ảnh gốc, đã được transform xử lý)\n",
        "    # boxes vẫn đang ở device\n",
        "    pooled = model.roi_heads.box_roi_pool(\n",
        "        features, [boxes], image_sizes)  # (N, 256, 7, 7)\n",
        "\n",
        "    # 8) box head -> (N, 1024)\n",
        "    # Đây là bước quan trọng để lấy feature vector 1024-D\n",
        "    box_repr = model.roi_heads.box_head(pooled)  # (N, 1024)\n",
        "\n",
        "    # 9) Tính toán và chuẩn hóa tọa độ\n",
        "    H_img, W_img = image_sizes[0]  # ints - kích thước ảnh sau khi transform\n",
        "    coords = compute_box_coords(boxes, (H_img, W_img), mode=coord_mode)\n",
        "    coords = coords.to(box_repr.dtype).to(\n",
        "        box_repr.device)  # Đảm bảo dtype và device khớp\n",
        "\n",
        "    # 10) Kết hợp đặc trưng và tọa độ -> (N, 1028)\n",
        "    feat1028 = torch.cat([box_repr, coords], dim=1)  # (N, 1028)\n",
        "\n",
        "    return feat1028.cpu()  # Chuyển về CPU để dễ xử lý sau này\n",
        "\n",
        "\n",
        "def aggregate_features(feature_list, target_num_features):\n",
        "    \"\"\"\n",
        "    Tổng hợp danh sách các tensor đặc trưng thành một tensor cố định.\n",
        "    Args:\n",
        "        feature_list: List of torch.Tensor, each with shape (N_i, 1028).\n",
        "        target_num_features: int, số lượng đặc trưng mong muốn.\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor với shape (target_num_features, 1028).\n",
        "    \"\"\"\n",
        "    if not feature_list:\n",
        "        print(\"Warning: feature_list is empty. Returning zero tensor.\")\n",
        "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
        "\n",
        "    # Gộp tất cả các đặc trưng từ các khung hình\n",
        "    # Bỏ qua các tensor rỗng (0, 1028) nếu có\n",
        "    non_empty_features = [f for f in feature_list if f.shape[0] > 0]\n",
        "\n",
        "    if not non_empty_features:\n",
        "        print(\"Warning: No features detected in any frame. Returning zero tensor.\")\n",
        "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
        "\n",
        "    # Shape: (Total_Detections, 1028)\n",
        "    all_features = torch.cat(non_empty_features, dim=0)\n",
        "    total_detections = all_features.shape[0]\n",
        "    print(\n",
        "        f\"Total object detections across all sampled frames: {total_detections}\")\n",
        "\n",
        "    if total_detections == 0:\n",
        "        return torch.zeros(target_num_features, 1028, dtype=torch.float32)\n",
        "\n",
        "    if total_detections >= target_num_features:\n",
        "        # Lấy mẫu đều để giảm xuống target_num_features\n",
        "        indices = np.linspace(0, total_detections - 1,\n",
        "                              target_num_features, dtype=int)\n",
        "        selected_features = all_features[indices]\n",
        "    else:\n",
        "        # Nếu không đủ, pad bằng cách lặp lại đặc trưng cuối cùng\n",
        "        print(\n",
        "            f\"Warning: Only {total_detections} features found. Padding to {target_num_features}.\")\n",
        "        num_to_pad = target_num_features - total_detections\n",
        "        if num_to_pad > 0:\n",
        "            # [num_to_pad, 1028]\n",
        "            padding = all_features[-1:].repeat(num_to_pad, 1)\n",
        "            # [target_num_features, 1028]\n",
        "            selected_features = torch.cat([all_features, padding], dim=0)\n",
        "        else:\n",
        "            selected_features = all_features\n",
        "\n",
        "    # Đảm bảo shape cuối cùng chính xác\n",
        "    assert selected_features.shape == (\n",
        "        target_num_features, 1028), f\"Aggregation failed: {selected_features.shape}\"\n",
        "    return selected_features  # Shape: (target_num_features, 1028)\n",
        "\n",
        "\n",
        "def extract_object_features_from_video(video_frames_bgr):\n",
        "    \"\"\"\n",
        "    Trích xuất đặc trưng đối tượng từ danh sách khung hình video (BGR numpy arrays).\n",
        "    Args:\n",
        "        video_frames_bgr: List of BGR numpy arrays.\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor đặc trưng có shape (1, TARGET_TOTAL_FEATURES, 1028).\n",
        "    \"\"\"\n",
        "\n",
        "    # Tạo placeholder cho relationship features, kích thước (50, 300)\n",
        "    relationship_feats = torch.zeros((TARGET_TOTAL_FEATURES, 300), dtype=torch.float32)\n",
        "\n",
        "    # Danh sách lưu trữ đặc trưng từ từng khung hình\n",
        "    all_frame_features = []\n",
        "\n",
        "    # Lặp qua từng khung hình BGR\n",
        "    print(\"Extracting object features using Mask R-CNN...\")\n",
        "    for i, frame_bgr in enumerate(video_frames_bgr):\n",
        "        # In mỗi 10 frames hoặc với i == 0\n",
        "        if (i + 1) % 10 == 0 or i == 0:\n",
        "            print(f\"  >> Processing frame {i+1}/{len(video_frames_bgr)}\")\n",
        "        # Chuyển đổi BGR (OpenCV) -> RGB -> PIL Image\n",
        "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        pil_img = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # Trích xuất đặc trưng từ khung hình\n",
        "        try:\n",
        "            frame_features = extract_instance_feats_with_coords(\n",
        "                maskrcnn_model, pil_img,\n",
        "                topk=TOP_K_PER_FRAME,\n",
        "                coord_mode=COORD_MODE,\n",
        "                conf_threshold=CONFIDENCE_THRESHOLD\n",
        "            )\n",
        "            # Thêm tensor (N_i, 1028) hoặc (0, 1028)\n",
        "            all_frame_features.append(frame_features)\n",
        "        except Exception as e:\n",
        "            print(f\"    Error extracting features from frame {i+1}: {e}\")\n",
        "            # Thêm tensor rỗng nếu lỗi\n",
        "            all_frame_features.append(\n",
        "                torch.empty((0, 1028), dtype=torch.float32))\n",
        "\n",
        "    # Tổng hợp đặc trưng để có số lượng cố định\n",
        "    print(\"[v] Aggregating features to target shape...\")\n",
        "    object_feats = aggregate_features(\n",
        "        all_frame_features, TARGET_TOTAL_FEATURES)  # Tensor (50, 1028)\n",
        "    print(f\"[v] Object feature extraction complete. Shape: {object_feats.shape}\")\n",
        "\n",
        "    return object_feats, relationship_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b844036",
      "metadata": {
        "id": "2b844036"
      },
      "source": [
        "## I3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "777d29c9",
      "metadata": {
        "id": "777d29c9"
      },
      "outputs": [],
      "source": [
        "# --- I3D model global để tránh tải lại nhiều lần ---\n",
        "i3d_model = None\n",
        "\n",
        "def extract_motion_features(video_path):\n",
        "    global i3d_model, FEATURE_MODE\n",
        "    if i3d_model is None:\n",
        "        print(\"Loading I3D model...\")\n",
        "        # Select the feature type\n",
        "        feature_type = 'i3d'\n",
        "\n",
        "        # Load and patch the config\n",
        "        args = OmegaConf.load(build_cfg_path(feature_type))\n",
        "        args.video_paths = [video_path]\n",
        "        # args.show_pred = True\n",
        "        args.stack_size = 64\n",
        "        args.step_size = 64\n",
        "        args.extraction_fps = 50\n",
        "        args.flow_type = 'raft'\n",
        "        # args.streams = 'flow'\n",
        "\n",
        "        # Load the model\n",
        "        i3d_model = ExtractI3D(args)\n",
        "        print(\">> I3D Model loaded.\")\n",
        "\n",
        "    # Extract motion features\n",
        "    print(\"Extracting motion features from video...\")\n",
        "    feature_dict = i3d_model.extract(video_path)\n",
        "\n",
        "    # Kết hợp đặc trưng RGB và Flow từ 2 stream\n",
        "    print(\"  >> Combining RGB and Flow features...\")\n",
        "    motion_feats = feature_dict['rgb'] + feature_dict['flow']\n",
        "\n",
        "    # Nếu thời lượng của video quá ngắn, đầu ra có shape (0,)\n",
        "    if motion_feats.shape[0] == 0:\n",
        "        # In thông báo\n",
        "        print(\"[!] Video too short, no motion features extracted. Returning zero tensor.\")\n",
        "        # Trả về tensor zero với shape (1, 1024) để tránh lỗi\n",
        "        motion_feats = np.zeros((1, 1024), dtype=np.float32)\n",
        "        # # Cập nhật lại FEATURE_MODE để tránh lỗi trong mô hình\n",
        "        # FEATURE_MODE = \"two\"\n",
        "        # print(f\"  >> Updated FEATURE_MODE to '{FEATURE_MODE}' to handle short video.\")\n",
        "\n",
        "    print(f\"[v] Motion feature extraction complete. Shape: {motion_feats.shape}\")\n",
        "\n",
        "    return motion_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074f4499",
      "metadata": {
        "id": "074f4499"
      },
      "source": [
        "## Full pipeline for feature extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1a7530a7",
      "metadata": {
        "id": "1a7530a7"
      },
      "outputs": [],
      "source": [
        "def sample_frames(video_path, target_fps):\n",
        "    \"\"\" Hàm lấy mẫu khung hình từ video ở fps mục tiêu.\n",
        "    Ảnh trả về là danh sách các mảng numpy (BGR).\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    interval = max(1, int(round(orig_fps / target_fps)))\n",
        "    frames = []\n",
        "    idx = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if idx % interval == 0:\n",
        "            # Giữ nguyên ở dạng BGR hoặc chuyển sang RGB nếu mô hình yêu cầu (InceptionResNetV2 dùng RGB)\n",
        "            # frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) # Nếu cần RGB\n",
        "            frames.append(frame)  # Giữ BGR, sẽ xử lý sau\n",
        "        idx += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def resample_fixed(feats, N):\n",
        "    \"\"\" Hàm lấy mẫu lại chuỗi đặc trưng để có đúng N đặc trưng bằng cách lấy mẫu đều.\n",
        "    \"\"\"\n",
        "    T, D = feats.shape\n",
        "    idxs = np.linspace(0, T-1, N).astype(int)\n",
        "    return feats[idxs]\n",
        "\n",
        "\n",
        "def process_video(video_path, dataset='MSVD'):\n",
        "    \"\"\" Full pipeline để xử lý video và trích xuất đặc trưng.\n",
        "    \"\"\"\n",
        "    N = 50 if dataset == 'MSVD' else 60\n",
        "    target_fps = 5 if dataset == 'MSVD' else 3\n",
        "\n",
        "    # --- Tải video và lấy mẫu frames ---\n",
        "    print(\"Sampling video frames...\")\n",
        "    # Danh sách các mảng numpy (BGR)\n",
        "    video_frames = sample_frames(video_path, target_fps)\n",
        "    print(f\"Sampled {len(video_frames)} frames at ~{target_fps} fps.\")\n",
        "\n",
        "    # Nếu có nhiều hơn 50 frames, lấy 50 frames cách đều nhau để giảm bớt\n",
        "    if len(video_frames) > 50:\n",
        "        indices = np.linspace(0, len(video_frames) - 1, 50, dtype=int)\n",
        "        video_frames = [video_frames[i] for i in indices]\n",
        "        print(f\"Reduced to {len(video_frames)} frames for processing.\")\n",
        "\n",
        "    # Trích xuất đặc trưng với mô hình Inception-ResNet-V2\n",
        "    image_feats = extract_image_features(video_frames)\n",
        "    # Trích xuất đặc trưng với mô hình Mask R-CNN\n",
        "    object_feats, rel_feats = extract_object_features_from_video(video_frames)\n",
        "    # Trích xuất đặc trưng với mô hình I3D\n",
        "    motion_feats = extract_motion_features(video_path)\n",
        "\n",
        "    # Resample để có đúng N đặc trưng\n",
        "    image_feats_res = resample_fixed(image_feats, N)  # (N, 1536)\n",
        "    object_feats_res = resample_fixed(object_feats.numpy(), N)  # (N, 1028)\n",
        "    rel_feats_res = resample_fixed(rel_feats.numpy(), N)  # (N, 300)\n",
        "    motion_feats_res = resample_fixed(motion_feats, N)  # (N, 1024)\n",
        "\n",
        "    # Đảm bảo tất cả đặc trưng đều có kiểu dữ liệu float32\n",
        "    image_feats_res = image_feats_res.astype(np.float32)\n",
        "    object_feats_res = object_feats_res.astype(np.float32)\n",
        "    rel_feats_res = rel_feats_res.astype(np.float32)\n",
        "    motion_feats_res = motion_feats_res.astype(np.float32)\n",
        "\n",
        "    # Chuyển sang tensor và thêm batch dim\n",
        "    image_feats_tensor = torch.from_numpy(image_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 1536)\n",
        "    object_feats_tensor = torch.from_numpy(object_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 1028)\n",
        "    rel_feats_tensor = torch.from_numpy(rel_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 300)\n",
        "    motion_feats_tensor = torch.from_numpy(motion_feats_res).to(\n",
        "        DEVICE).unsqueeze(0)  # (1, N, 1024)\n",
        "\n",
        "    # Trả về kết quả\n",
        "    return {\n",
        "        'image_feats': image_feats_tensor,\n",
        "        'motion_feats': motion_feats_tensor,\n",
        "        'object_feats': object_feats_tensor,\n",
        "        'rel_feats': rel_feats_tensor\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8331fd7c",
      "metadata": {
        "id": "8331fd7c"
      },
      "source": [
        "# Load checkpoint and config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4281485a",
      "metadata": {
        "id": "4281485a"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"checkpoints/best.ckpt\", map_location=\"cpu\")\n",
        "config = dict_to_cls(checkpoint['config'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "86d47827",
      "metadata": {
        "id": "86d47827"
      },
      "outputs": [],
      "source": [
        "corpus = MSVD(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2d87b1",
      "metadata": {
        "id": "ec2d87b1"
      },
      "source": [
        "# Build Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "65f8ff2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65f8ff2e",
        "outputId": "1c45fbb7-83af-4c1f-d34b-0853f058beae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "vocab = corpus.vocab\n",
        "\"\"\" Build Models \"\"\"\n",
        "try:\n",
        "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
        "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
        "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big,\n",
        "                           select_num=config.transformer.select_num)\n",
        "except:\n",
        "    model = ABDTransformer(vocab, config.feat.size, config.transformer.d_model, config.transformer.d_ff,\n",
        "                           config.transformer.n_heads, config.transformer.n_layers, config.transformer.dropout,\n",
        "                           config.feat.feature_mode, n_heads_big=config.transformer.n_heads_big)\n",
        "model.load_state_dict(checkpoint['abd_transformer'])\n",
        "model.device = DEVICE\n",
        "model.feature_mode = FEATURE_MODE\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(DEVICE)\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44af2738",
      "metadata": {
        "id": "44af2738"
      },
      "source": [
        "# Inference with beam search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fe3d06f6",
      "metadata": {
        "id": "fe3d06f6"
      },
      "outputs": [],
      "source": [
        "def generate_video_caption(video_path):\n",
        "    global FEATURE_MODE\n",
        "    # Reset feature mode in case it was changed\n",
        "    FEATURE_MODE = \"three\"\n",
        "\n",
        "    # Rút trích đặc trưng từ video\n",
        "    feats_dict = process_video(video_path)\n",
        "    feats = (\n",
        "        feats_dict['image_feats'],\n",
        "        feats_dict['motion_feats'],\n",
        "        feats_dict['object_feats'],\n",
        "        feats_dict['rel_feats']\n",
        "    )\n",
        "    # Tạo caption cho video\n",
        "    model.eval()\n",
        "    model.feature_mode = FEATURE_MODE\n",
        "    print(f\"Generating caption with feature mode: {model.feature_mode}\")\n",
        "    beam_size = config.beam_size\n",
        "    max_len = config.loader.max_caption_len\n",
        "    with torch.no_grad():\n",
        "        r2l_captions, l2r_captions = model.beam_search_decode(feats, beam_size, max_len)\n",
        "        # r2l_captions = [idxs_to_sentence(caption, vocab.idx2word, BOS_idx) for caption in r2l_captions]\n",
        "        l2r_captions = [\" \".join(caption[0].value) for caption in l2r_captions]\n",
        "        r2l_captions = [\" \".join(caption[0].value) for caption in r2l_captions]\n",
        "\n",
        "    print(f\"Left to Right Captions: {l2r_captions}\")\n",
        "    return l2r_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cf74303e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf74303e",
        "outputId": "6d8d4891-fb21-4f68-eda0-5ad6c9417eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating caption for lifting.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 89 frames at ~5 fps.\n",
            "Reduced to 50 frames for processing.\n",
            "Extracting image features using timm Inception-ResNet-V2...\n",
            "  >> Extract image feature for frame 1/50\n",
            "  >> Extract image feature for frame 10/50\n",
            "  >> Extract image feature for frame 20/50\n",
            "  >> Extract image feature for frame 30/50\n",
            "  >> Extract image feature for frame 40/50\n",
            "  >> Extract image feature for frame 50/50\n",
            "[v] Image feature extraction complete. Shape: (50, 1536)\n",
            "Extracting object features using Mask R-CNN...\n",
            "  >> Processing frame 1/50\n",
            "  >> Processing frame 10/50\n",
            "  >> Processing frame 20/50\n",
            "  >> Processing frame 30/50\n",
            "  >> Processing frame 40/50\n",
            "  >> Processing frame 50/50\n",
            "[v] Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 58\n",
            "[v] Object feature extraction complete. Shape: torch.Size([50, 1028])\n",
            "Loading I3D model...\n",
            ">> I3D Model loaded.\n",
            "Extracting motion features from video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  >> Combining RGB and Flow features...\n",
            "[v] Motion feature extraction complete. Shape: (14, 1024)\n",
            "Generating caption with feature mode: three\n",
            "Left to Right Captions: ['a man is lifting weights']\n",
            ">> Time taken: 93.28 seconds\n",
            ">> Captions for lifting.mp4: ['a man is lifting weights']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for bento_3s.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 15 frames at ~5 fps.\n",
            "Extracting image features using timm Inception-ResNet-V2...\n",
            "  >> Extract image feature for frame 1/15\n",
            "  >> Extract image feature for frame 10/15\n",
            "[v] Image feature extraction complete. Shape: (15, 1536)\n",
            "Extracting object features using Mask R-CNN...\n",
            "  >> Processing frame 1/15\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  >> Processing frame 10/15\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "[v] Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 34\n",
            "Warning: Only 34 features found. Padding to 50.\n",
            "[v] Object feature extraction complete. Shape: torch.Size([50, 1028])\n",
            "Extracting motion features from video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  >> Combining RGB and Flow features...\n",
            "[v] Motion feature extraction complete. Shape: (2, 1024)\n",
            "Generating caption with feature mode: three\n",
            "Left to Right Captions: ['a man is slicing a stick']\n",
            ">> Time taken: 17.51 seconds\n",
            ">> Captions for bento_3s.mp4: ['a man is slicing a stick']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for bento_2s.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 10 frames at ~5 fps.\n",
            "Extracting image features using timm Inception-ResNet-V2...\n",
            "  >> Extract image feature for frame 1/10\n",
            "  >> Extract image feature for frame 10/10\n",
            "[v] Image feature extraction complete. Shape: (10, 1536)\n",
            "Extracting object features using Mask R-CNN...\n",
            "  >> Processing frame 1/10\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "Warning: No objects detected (or survived filtering). Returning empty features.\n",
            "  >> Processing frame 10/10\n",
            "[v] Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 14\n",
            "Warning: Only 14 features found. Padding to 50.\n",
            "[v] Object feature extraction complete. Shape: torch.Size([50, 1028])\n",
            "Extracting motion features from video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n",
            "/content/drive/MyDrive/btkg/models/raft/raft_src/raft.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  >> Combining RGB and Flow features...\n",
            "[v] Motion feature extraction complete. Shape: (1, 1024)\n",
            "Generating caption with feature mode: three\n",
            "Left to Right Captions: ['a man is slicing a potato']\n",
            ">> Time taken: 10.99 seconds\n",
            ">> Captions for bento_2s.mp4: ['a man is slicing a potato']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating caption for bento_1s.mp4...\n",
            "Sampling video frames...\n",
            "Sampled 5 frames at ~5 fps.\n",
            "Extracting image features using timm Inception-ResNet-V2...\n",
            "  >> Extract image feature for frame 1/5\n",
            "[v] Image feature extraction complete. Shape: (5, 1536)\n",
            "Extracting object features using Mask R-CNN...\n",
            "  >> Processing frame 1/5\n",
            "[v] Aggregating features to target shape...\n",
            "Total object detections across all sampled frames: 12\n",
            "Warning: Only 12 features found. Padding to 50.\n",
            "[v] Object feature extraction complete. Shape: torch.Size([50, 1028])\n",
            "Extracting motion features from video...\n",
            "  >> Combining RGB and Flow features...\n",
            "[!] Video too short, no motion features extracted. Returning zero tensor.\n",
            "[v] Motion feature extraction complete. Shape: (1, 1024)\n",
            "Generating caption with feature mode: three\n",
            "Left to Right Captions: ['a woman is putting a pepper']\n",
            ">> Time taken: 2.94 seconds\n",
            ">> Captions for bento_1s.mp4: ['a woman is putting a pepper']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Duyệt qua từng video trong folder 'videos' và tạo caption\n",
        "import os\n",
        "for filename in os.listdir('videos'):\n",
        "    video_path = os.path.join('videos', filename)\n",
        "    print(f\"Generating caption for {filename}...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    captions = generate_video_caption(video_path)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\">> Time taken: {end_time - start_time:.2f} seconds\")\n",
        "    print(f\">> Captions for {filename}: {captions}\\n\")\n",
        "    print(\"--------------------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Z2BBhrfwsXKW",
      "metadata": {
        "id": "Z2BBhrfwsXKW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "985f4559",
        "5ad625c4",
        "2b844036",
        "074f4499"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ef68951ebd0447ba737d511cc312b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23fb3ea7405d4815a8a43cbe9256d4af",
              "IPY_MODEL_8ed456d00de5480fbdc4acb4eb6492ab",
              "IPY_MODEL_3a3de7daffb042a48dfdfae819becfc6"
            ],
            "layout": "IPY_MODEL_190aa24befec423abc7aadb11b71abd5"
          }
        },
        "23fb3ea7405d4815a8a43cbe9256d4af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_524dfc225c8a40b3aeb4d399db95cd28",
            "placeholder": "​",
            "style": "IPY_MODEL_f1abe7f4ea8149848db27d6f89056f3e",
            "value": "model.safetensors: 100%"
          }
        },
        "8ed456d00de5480fbdc4acb4eb6492ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2418933a2dd341b6ac8137be6c1f93da",
            "max": 223748230,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b8159df80d544b8a5e763b2d356d7a5",
            "value": 223748230
          }
        },
        "3a3de7daffb042a48dfdfae819becfc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85b54471a4e0403a9860fe06c77f2fdc",
            "placeholder": "​",
            "style": "IPY_MODEL_c62cafce06b64c12831feecfe210310f",
            "value": " 224M/224M [00:00&lt;00:00, 283MB/s]"
          }
        },
        "190aa24befec423abc7aadb11b71abd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "524dfc225c8a40b3aeb4d399db95cd28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1abe7f4ea8149848db27d6f89056f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2418933a2dd341b6ac8137be6c1f93da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8159df80d544b8a5e763b2d356d7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85b54471a4e0403a9860fe06c77f2fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62cafce06b64c12831feecfe210310f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}